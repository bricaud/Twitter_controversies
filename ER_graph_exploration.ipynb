{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import json\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pysad\n",
    "import pysad.utils\n",
    "import pysad.collect\n",
    "import pysad.synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.collect)\n",
    "importlib.reload(pysad.synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_nodes = 200\n",
    "nb_edges = 500\n",
    "G = nx.gnm_random_graph(nb_nodes, nb_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_handle = pysad.synthesis.graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_depth = 5 # mention of mention of mention of ... up to exploration depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_user_list, total_nodes_df, total_edges_df = pysad.collect.spiky_ball([20], \n",
    "                                                                               graph_handle, \n",
    "                                                                               exploration_depth=exploration_depth,\n",
    "                                                                               random_subset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of users mentioned:',len(total_user_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysad.graph\n",
    "import pysad.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_df, edge_df = total_nodes_df, total_edges_df\n",
    "node_df = pysad.graph.reshape_node_data(node_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.graph)\n",
    "importlib.reload(pysad.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WEIGHT = 2\n",
    "MIN_DEGREE = 2 # Minimal number of connections in the graph\n",
    "\n",
    "G = pysad.graph.graph_from_edgeslist(edge_df, MIN_WEIGHT)\n",
    "#G = pysad.graph.graph_from_edgeslist(df_pop,DEGREE_MIN)\n",
    "G = pysad.graph.add_node_attributes(G,node_df)\n",
    "G = pysad.graph.reduce_graph(G,MIN_DEGREE)\n",
    "# Warning, graph properties are not saved by networkx in gexf files except graph name\n",
    "G.graph['end_date'] = max(edge_df['date']).strftime(\"%d/%m/%Y\") \n",
    "G.graph['start_date'] = min(edge_df['date']).strftime(\"%d/%m/%Y\")\n",
    "G.graph['name'] = category_name + ' ' + G.graph['start_date'] + ' - ' + G.graph['end_date'] \n",
    "print('Period from {} to {}.'.format(G.graph['start_date'],G.graph['end_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the info of the nodes not collected\n",
    "nodes_missing_info = [node for node,data in G.nodes(data=True) if 'name' not in data]\n",
    "print('Number of nodes with missing info:',len(nodes_missing_info))\n",
    "\n",
    "# 2 options: 1) remove nodes with missing info or 2) rerun the collection to collect the missing info\n",
    "option = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if option == 1:\n",
    "    # Option 1:\n",
    "    print('Removing node with missing info from the graph')\n",
    "    G.remove_nodes_from(nodes_missing_info)\n",
    "    print('Number of nodes after removal:',G.number_of_nodes())\n",
    "else:\n",
    "    # Option 2: collect the missing node data\n",
    "    nodeinfo_df = pd.DataFrame()\n",
    "    nb_missing = len(nodes_missing_info)\n",
    "    for idx,node in enumerate(nodes_missing_info):\n",
    "        print('collecting info for {} - {} / {} '.format(node,idx,nb_missing))\n",
    "        edgeinfo,nodeinfo = pysad.collect.collect_user_data(node,python_tweets,max_day_old)\n",
    "        nodeinfo_df = nodeinfo_df.append(nodeinfo)\n",
    "\n",
    "    nodeinfo_df = pysad.graph.reshape_node_data(nodeinfo_df)\n",
    "    G = pysad.graph.add_node_attributes(G,nodeinfo_df)\n",
    "    print('Node info added to the graph.')\n",
    "# Check integrity\n",
    "i=0\n",
    "for node,data in G.nodes(data=True):\n",
    "    if 'name' not in data:\n",
    "        print('Missing information for',node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(G.degree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection to get the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.clusters)\n",
    "importlib.reload(pysad.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G,clusters = pysad.graph.detect_communities(G)\n",
    "G.nb_communities = len(clusters)\n",
    "#c_connectivity = pysad.clusters.cluster_connectivity(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = pysad.graph.remove_small_communities(G,clusters,min_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph\n",
    "import networkx as nx\n",
    "import json\n",
    "# Save as gexf file\n",
    "min_mentions = graph_handle.rules['min_mentions']\n",
    "graphname = '' + category_name\n",
    "graphfilename = results_data_path + graphname + '_t' + str(min_mentions) + '_md' + str(MIN_DEGREE) +'_graph.gexf'\n",
    "jsongraphfilename = results_data_path + graphname + '_t' + str(min_mentions) + '_md' + str(MIN_DEGREE) +'_graph.json'\n",
    "nx.write_gexf(G,graphfilename)\n",
    "print('Wrote',graphfilename)\n",
    "\n",
    "# Save as json file\n",
    "Gnld = nx.readwrite.json_graph.node_link_data(G)\n",
    "with open(jsongraphfilename, 'w') as outfile:\n",
    "    json.dump(Gnld, outfile)\n",
    "print('Wrote',jsongraphfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c_connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.clusters)\n",
    "importlib.reload(pysad.utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic processing of all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the data from the clusters\n",
    "cluster_info_dic = {}\n",
    "for c_id in clusters:\n",
    "    cgraph = clusters[c_id]\n",
    "    if cgraph.number_of_nodes()==0: #in case a cluster has been removed\n",
    "        cluster_info_dic[c_id] = {}\n",
    "        continue\n",
    "    cgraph = pysad.clusters.cluster_attributes(cgraph)\n",
    "    table_dic = pysad.clusters.cluster_tables(cgraph)\n",
    "    #node_details = \n",
    "    cluster_filename = results_data_path + 'cluster' + str(c_id)\n",
    "    cluster_info_dic[c_id] = {}\n",
    "    cluster_info_dic[c_id]['info_table'] = table_dic\n",
    "    #cluster_info_dic[c_id]['user_details'] = node_details\n",
    "    cluster_info_dic[c_id]['filename'] = cluster_filename    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding global infos\n",
    "# keywords\n",
    "corpus = pysad.clusters.get_corpus(cluster_info_dic)\n",
    "keyword_dic = pysad.clusters.tfidf(corpus)\n",
    "# save in the cluster info dic\n",
    "for c_id in clusters:\n",
    "    if clusters[c_id].number_of_nodes()>0:\n",
    "        cluster_info_dic[c_id]['info_table']['keywords'] = keyword_dic[c_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering global info\n",
    "# Saving in excel files\n",
    "for c_id in cluster_info_dic:\n",
    "    if not cluster_info_dic[c_id]:\n",
    "        continue\n",
    "    info_table = cluster_info_dic[c_id]['info_table']\n",
    "    cluster_general_info = {'cluster id': c_id, 'Nb users': clusters[c_id].number_of_nodes(), \n",
    "                           'Nb of tweets':clusters[c_id].size(weight='weight'),\n",
    "                           'Start date': str(G.graph['start_date']),\n",
    "                           'End date': str(G.graph['end_date']),\n",
    "                           'Search topic': category_name}\n",
    "                           #'cluster connectivity': c_connectivity[c_id]}\n",
    "    cluster_general_df = pd.DataFrame.from_dict([cluster_general_info])\n",
    "    #info_table = {'cluster':cluster_general_df, **info_table}\n",
    "    sheet1 = pd.concat([cluster_general_df,info_table['hashtags'],info_table['keywords']],axis=1)\n",
    "    tweet_table = info_table['text']\n",
    "    #user_table = \n",
    "    cluster_indicators = pd.DataFrame([pysad.clusters.compute_cluster_indicators(clusters[c_id])])\n",
    "    excel_data = {'cluster':sheet1, 'tweets':tweet_table, 'indicators': cluster_indicators, 'users': node_df}\n",
    "    #excel_data = info_table\n",
    "    pysad.clusters.save_excel(excel_data,cluster_info_dic[c_id]['filename'] + '_infos.xlsx', table_format='Fanny')\n",
    "    pysad.graph.save_graph(clusters[c_id],cluster_info_dic[c_id]['filename'] + 'graph.gexf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving clusters info to be displayed with the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing clusters info to the graph\n",
    "G = pysad.clusters.clutersprop2graph(G,cluster_info_dic,clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph as a json file\n",
    "import networkx as nx\n",
    "\n",
    "graphname = 'graph'\n",
    "jsongraphfilename = results_data_path + graphname + '_t' + str(min_mentions) + '_md' + str(MIN_DEGREE) +'_graph.json'\n",
    "\n",
    "Gnld = nx.readwrite.json_graph.node_link_data(G)\n",
    "with open(jsongraphfilename, 'w') as outfile:\n",
    "    json.dump(Gnld, outfile)\n",
    "print('Wrote',jsongraphfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clusters info as a json file\n",
    "clusterinfotoviz = G.graph['clusters']\n",
    "jsonfilename = results_data_path + graphname + '_t' + str(min_mentions) + '_md' + str(MIN_DEGREE) +'_clusters.json'\n",
    "\n",
    "with open(jsonfilename, 'w') as outfile:\n",
    "    json.dump(clusterinfotoviz, outfile)\n",
    "print('Wrote',jsonfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
