{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import json\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Twython class\n",
    "from twython import Twython\n",
    "import json\n",
    "\n",
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)\n",
    "\n",
    "# Instantiate an object\n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pysad\n",
    "import pysad.utils\n",
    "import pysad.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pysad.collect' from '/home/benjamin/Documents/EPFL/Research/sad/sad_tweets/pysad/collect.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_accounts = pysad.utils.initial_accounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swiss_climate_regular',\n",
       " 'swiss_climate_controversial',\n",
       " 'swiss_immigration',\n",
       " 'french_tech_lesechos',\n",
       " 'swiss_immigration2',\n",
       " 'debat_burqa',\n",
       " 'coronavirus',\n",
       " 'coronavirus_iran',\n",
       " 'hackathlon',\n",
       " 'hackathlon_popular',\n",
       " 'hackathlon_reddit',\n",
       " 'hackathlon_missingtweets']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_accounts.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date string = 20200411\n"
     ]
    }
   ],
   "source": [
    "######Choose a category##############    \n",
    "#category_name = 'swiss_climate_controversial'\n",
    "#category_name = 'swiss_climate_regular'\n",
    "#category_name = 'french_tech_lesechos'\n",
    "#category_name = 'swiss_immigration'\n",
    "#category_name = 'swiss_immigration2'\n",
    "#category_name = 'debat_burqa'\n",
    "category_name = 'hackathlon'\n",
    "category_name = 'hackathlon_popular'\n",
    "category_name = 'hackathlon_reddit'\n",
    "category_name = 'hackathlon_missingtweets'\n",
    "\n",
    "#####################################\n",
    "\n",
    "username_list = init_accounts.accounts(category_name)\n",
    "\n",
    "# create the path to save the experiment indexed with the date of today\n",
    "today = date.today()\n",
    "date_string = today.strftime(\"%Y%m%d\")\n",
    "print(\"date string =\", date_string)\n",
    "\n",
    "tweet_data_path_list = ['tweetdata', category_name, date_string]\n",
    "results_data_path_list = ['resultsdata2', category_name, date_string]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_path = ''.join(tweet_data_path_list)\n",
    "results_data_path = ''.join(results_data_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize folders (create or clean them if they exist)\n",
    "# Set erase=False if you need to keep the previous collection\n",
    "tweet_data_path = pysad.utils.initialize_folder(tweet_data_path_list, erase=False)\n",
    "results_data_path = pysad.utils.initialize_folder(results_data_path_list, erase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the list of users is too large:\n",
    "# Keep a random subset of the list of users\n",
    "import random\n",
    "username_list_tmp = random.sample(username_list,len(username_list)//10)\n",
    "username_list = username_list_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(username_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mentions = 2 # minimal number of mentions of a user to be followed\n",
    "max_day_old = None # number max of days in the past\n",
    "exploration_depth = 1 # mention of mention of mention of ... up to exploration depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_user_list = pysad.collect.collect_tweets(username_list, tweet_data_path, python_tweets, min_mentions=min_mentions,\n",
    "               max_day_old=max_day_old, exploration_depth=exploration_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_user_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Loading the saved data into an edge table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweetdata/hackathlon_missingtweets/20200411/Alicia_Smith19_mentions.json with 41 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/nygovcuomo_mentions.json with 32 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/blue500000_mentions.json with 1 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/nytimes_mentions.json with 25 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/AskAKorean_mentions.json with 64 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/BFMTV_mentions.json with 24 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/m_gramatyka_mentions.json with 39 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/HadiNili_mentions.json with 75 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/jasminjoestar_mentions.json with 89 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/Freaktesam_mentions.json with 64 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/_pbls_mentions.json with 170 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/itrulyknowchina_mentions.json with 51 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/achinastory1_mentions.json with 137 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/db95139159_mentions.json with 92 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/justsmallprobs_mentions.json with 266 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/MuradGazdiev_mentions.json with 86 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/TheFoxintheWell_mentions.json with 5 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/lakecomoexpat_mentions.json with 41 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/campbell_kang_mentions.json with 70 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/Conflits_FR_mentions.json with 14 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/Brasco_Aad_mentions.json with 114 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/DarrenPlymouth_mentions.json with 87 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/BNODesk_mentions.json with 67 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/globaltimesnews_mentions.json with 4 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/ARanganathan72_mentions.json with 89 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/bnodesk_mentions.json with 67 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/jenniferatntd_mentions.json with 49 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/ericabuddington_mentions.json with 81 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/WHO_mentions.json with 96 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/GuidoVin_mentions.json with 108 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/RachelDonadio_mentions.json with 108 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/AsYouNotWish_mentions.json with 13 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/pulte_mentions.json with 32 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/mikiebarb_mentions.json with 118 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/oyyao_mentions.json with 201 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/GersPardoel_mentions.json with 110 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/bendreyfuss_mentions.json with 86 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/PeterDiamandis_mentions.json with 91 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/BirdieBangkok_mentions.json with 152 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/stereophonics_mentions.json with 80 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/EpiEllie_mentions.json with 124 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/SSaludCdMx_mentions.json with 122 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/ndawsari_mentions.json with 94 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/DrEricDing_mentions.json with 59 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/JonRead15_mentions.json with 43 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/MohamadAhwaze_mentions.json with 24 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/FYang_EP_mentions.json with 256 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/axios_mentions.json with 14 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/drericding_mentions.json with 59 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/SeeMeBe_mentions.json with 213 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/elcomerciocom_mentions.json with 9 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/LilyNCali_mentions.json with 146 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/gbotwinick_mentions.json with 114 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/mitchellvii_mentions.json with 12 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/realPhillipHale_mentions.json with 158 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/PDChina_mentions.json with 3 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/hopeseekr_mentions.json with 128 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/ezracheungtoto_mentions.json with 97 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/charlie15112871_mentions.json with 1 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/Breaking911_mentions.json with 6 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/scott_mintzer_mentions.json with 151 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/Zhao_Dading_mentions.json with 119 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/smilinondabeach_mentions.json with 208 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/fyang_ep_mentions.json with 256 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/aniilai_mentions.json with 79 tweets.\n",
      "tweetdata/hackathlon_missingtweets/20200411/eric62643655_mentions.json with 6 tweets.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "edge_df = pd.DataFrame()\n",
    "#for filename in glob.glob(tweet_data_path + '*_mentions' +'_t' +str(min_mentions)+ '.json'):\n",
    "for filename in glob.glob(tweet_data_path + '*_mentions*' + '.json'):\n",
    "    new_edge_df = pd.read_json(filename)\n",
    "    print('{} with {} tweets.'.format(filename,len(new_edge_df)))\n",
    "    edge_df = edge_df.append(new_edge_df)\n",
    "edge_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the most popular\n",
    "df_pop = edge_df[edge_df['retweet_count'] > 1000]\n",
    "df_pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysad.graph\n",
    "import pysad.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pysad.graph' from '/home/benjamin/Documents/EPFL/Research/sad/sad_tweets/pysad/graph.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the graph from the edge list\n",
      "Nb of nodes: 2769\n",
      "Nb of nodes after removing nodes with degree strictly smaller than 2: 186\n",
      "removed 5 isolated nodes.\n",
      "Warning: the graph is directed.\n",
      "Period from 2016-07-27 12:40:55 to 2020-04-11 13:36:55.\n"
     ]
    }
   ],
   "source": [
    "DEGREE_MIN = 2 # Minimal number of connections in the graph\n",
    "\n",
    "G = pysad.graph.graph_from_edgeslist(edge_df,DEGREE_MIN)\n",
    "#G = pysad.graph.graph_from_edgeslist(df_pop,DEGREE_MIN)\n",
    "G.name = category_name\n",
    "G.end_date = max(edge_df['date']) #max(edge_df['date'].apply(max))\n",
    "G.start_date = min(edge_df['date']) #min(edge_df['date'].apply(min))\n",
    "print('Period from {} to {}.'.format(G.start_date,G.end_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection to get the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pysad.graph' from '/home/benjamin/Documents/EPFL/Research/sad/sad_tweets/pysad/graph.py'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad.clusters)\n",
    "importlib.reload(pysad.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communities saved on the graph as node attributes.\n",
      "Nb of partitions: 10\n"
     ]
    }
   ],
   "source": [
    "G,clusters = pysad.graph.detect_communities(G)\n",
    "G.nb_communities = len(clusters)\n",
    "c_connectivity = pysad.clusters.cluster_connectivity(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 0 community(ies) smaller than 5 nodes.\n"
     ]
    }
   ],
   "source": [
    "G = pysad.graph.remove_small_communities(G,clusters,min_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph\n",
    "import networkx as nx\n",
    "\n",
    "graphname = 'missinggraph_reddit'\n",
    "graphfilename = results_data_path + graphname + '_t' + str(min_mentions) + '_md' + str(DEGREE_MIN) +'_graph.gexf'\n",
    "nx.write_gexf(G,graphfilename)\n",
    "print('Wrote',graphfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.344,\n",
       " 1: 0.17,\n",
       " 2: 0.341,\n",
       " 3: 0.61,\n",
       " 4: 0.338,\n",
       " 5: 0.35,\n",
       " 6: 0.139,\n",
       " 7: 0.333,\n",
       " 8: 0.359,\n",
       " 9: 0.295}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pysad' from '/home/benjamin/Documents/EPFL/Research/sad/sad_tweets/pysad/__init__.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pysad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic processing of all clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the data from the clusters\n",
    "cluster_info_dic = {}\n",
    "for c_id in clusters:\n",
    "    cgraph = clusters[c_id]\n",
    "    cgraph = pysad.clusters.cluster_attributes(cgraph)\n",
    "    table_dic = pysad.clusters.cluster_tables(cgraph)\n",
    "    cluster_filename = results_data_path + 'cluster' + str(c_id)\n",
    "    cluster_info_dic[c_id] = {}\n",
    "    cluster_info_dic[c_id]['info_table'] = table_dic\n",
    "    cluster_info_dic[c_id]['filename'] = cluster_filename    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding global infos\n",
    "# keywords\n",
    "corpus = pysad.clusters.get_corpus(cluster_info_dic)\n",
    "keyword_dic = pysad.clusters.tfidf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster0_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster0graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster1_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster1graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster2_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster2graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster3_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster3graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster4_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster4graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster5_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster5graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster6_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster6graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster7_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster7graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster8_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster8graph.gexf\n",
      "Data saved to resultsdata2/hackathlon_missingtweets/20200411/cluster9_infos.xlsx\n",
      "Graph saved to resultsdata2/hackathlon_missingtweets/20200411/cluster9graph.gexf\n"
     ]
    }
   ],
   "source": [
    "# gathering global info\n",
    "# Saving in excel files\n",
    "for c_id in cluster_info_dic:\n",
    "    info_table = cluster_info_dic[c_id]['info_table']\n",
    "    info_table['keywords'] = keyword_dic[c_id]\n",
    "    cluster_general_info = {'cluster id': c_id, 'Nb users': clusters[c_id].number_of_nodes(), \n",
    "                           'Nb of tweets':clusters[c_id].size(weight='weight'),\n",
    "                           'Start date': str(G.start_date),\n",
    "                           'End date': str(G.end_date),\n",
    "                           'Search topic': category_name,\n",
    "                           'cluster connectivity': c_connectivity[c_id]}\n",
    "    cluster_general_df = pd.DataFrame.from_dict([cluster_general_info])\n",
    "    #info_table = {'cluster':cluster_general_df, **info_table}\n",
    "    sheet1 = pd.concat([cluster_general_df,info_table['hashtags'],info_table['keywords']],axis=1)\n",
    "    tweet_table = info_table['text']\n",
    "    cluster_indicators = pd.DataFrame([pysad.clusters.compute_cluster_indicators(clusters[c_id])])\n",
    "    excel_data = {'cluster':sheet1, 'tweets':tweet_table, 'indicators': cluster_indicators}\n",
    "    #excel_data = info_table\n",
    "    pysad.clusters.save_excel(excel_data,cluster_info_dic[c_id]['filename'] + '_infos.xlsx', table_format='Fanny')\n",
    "    pysad.graph.save_graph(clusters[c_id],cluster_info_dic[c_id]['filename'] + 'graph.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
