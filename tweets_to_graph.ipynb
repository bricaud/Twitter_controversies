{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Twython class\n",
    "from twython import Twython\n",
    "import json\n",
    "\n",
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)\n",
    "\n",
    "# Instantiate an object\n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date string = 191213\n"
     ]
    }
   ],
   "source": [
    "#username = 'templivs'\n",
    "#username_list  = ['GilbertCollard','dav_dec','Carbongate','bcassoret',\n",
    "#                  'Electroversenet','thinkfree55', 'KlassLib','sauvonsleclimat']\n",
    "\n",
    "username_list = ['francisrichard','MazdaArtaxerxes','templivs','prontipronto',\n",
    "                'Chabadalala','cocktail2Funk','HopitalC',\n",
    "                'riva_vitale','Remifasol57','AitiDouze', 'QAnonAustria1', 'gotteswerk2411']\n",
    "\n",
    "swiss_accounts = ['KlimaschutzCH', 'GrueneCH', 'proclimCH', 'EperonP', 'MathiasTemujin',\n",
    "                  'klimastreik', 'AlimEquitables', 'ProNaturaSuisse', 'vertliberaux', 'Munsterma',\n",
    "                  'bourg_d', 'LesVertsSuisses', 'ClimatSuisse', 'gpsuisse', 'IliasPanchard', 'ATE_Suisse']\n",
    "\n",
    "username_list += swiss_accounts\n",
    "# create the path to save the experiment indexed with a date\n",
    "today = date.today()\n",
    "date_string = today.strftime(\"%y%m%d\")\n",
    "print(\"date string =\", date_string)\n",
    "\n",
    "#date_string = '191128'\n",
    "\n",
    "data_path = 'multiusers' + date_string+ '/'\n",
    "#get_tweets = python_tweets.get_user_timeline(screen_name = username,  \n",
    "#                                           count = 200, include_rts = True)\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n",
    "    print('Path created:',data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pysad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 2 # minimal number of mentions to keep\n",
    "max_day_old = 2 # number max of days in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_list(python_tweets, data_path, username, thres=3, max_day_old=None):\n",
    "    users_dic = {'username':[], 'Nb_diff_mentions': []}\n",
    "    print('Collecting the tweets for the last {} days.'.format(max_day_old))\n",
    "    new_users_list = []\n",
    "    for user in username_list:\n",
    "        mentions = create_user_edgelist(python_tweets, data_path, user, thres=thres, max_day_old=max_day_old)\n",
    "        if not mentions.empty:\n",
    "            users_mentioned = mentions['mention'][mentions['weight']>thres]\n",
    "            #users_mentioned = users_mentioned.unique() # not sure this is useful\n",
    "            new_users_list += users_mentioned.tolist()\n",
    "        users_dic['username'].append(user)\n",
    "        users_dic['Nb_diff_mentions'].append(len(mentions))\n",
    "    users_df = pd.DataFrame(users_dic)\n",
    "    return new_users_list,users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_user_edgelist(python_tweets, data_path, username, thres, max_day_old):\n",
    "\t# Process the user username and its mentioned users\n",
    "\t# save in a file the edgelist for the user and each mentioned user\n",
    "\n",
    "\t# initial user\n",
    "\tprint('Processing',username)\n",
    "\t#try:\n",
    "\tmention_grouped,mgl = pysad.collect_user_mention(username,python_tweets,data_path, max_day_old=max_day_old)\n",
    "\t#except:\n",
    "\t#    print('exception catched on user {} !!!!!!!!!!!!'.format(username))\n",
    "\t#    return\n",
    "\tif mention_grouped.empty:\n",
    "\t\tprint('Empty tweet list. Processing stopped for user ',username)\n",
    "\t\treturn mention_grouped\n",
    "\tmentionfilename = data_path + username + '_mentions' +'_t' +str(thres)+'.csv'\n",
    "\tprint('Writing {} tweets in {}.'.format(len(mention_grouped),mentionfilename))\n",
    "\tmention_grouped.to_csv(mentionfilename)\n",
    "\t#nb_mentions = len(mention_grouped)\n",
    "\t#print('User {} done. Nb different mentions: {}'.format(username,nb_mentions))\n",
    "\treturn mention_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting the tweets for the last 2 days.\n",
      "\n",
      "******* Processing users at 0-hop distance *******\n",
      "Collecting the tweets for the last 2 days.\n",
      "Processing francisrichard\n",
      "Empty tweet list. Processing stopped for user  francisrichard\n",
      "Processing MazdaArtaxerxes\n",
      "Empty tweet list. Processing stopped for user  MazdaArtaxerxes\n",
      "Processing templivs\n",
      "Empty tweet list. Processing stopped for user  templivs\n",
      "Processing prontipronto\n",
      "Writing 12 tweets in multiusers191213/prontipronto_mentions_t2.csv.\n",
      "Processing Chabadalala\n",
      "Writing 91 tweets in multiusers191213/Chabadalala_mentions_t2.csv.\n",
      "Processing cocktail2Funk\n",
      "Writing 5 tweets in multiusers191213/cocktail2Funk_mentions_t2.csv.\n",
      "Processing HopitalC\n",
      "Writing 33 tweets in multiusers191213/HopitalC_mentions_t2.csv.\n",
      "Processing riva_vitale\n",
      "Writing 3 tweets in multiusers191213/riva_vitale_mentions_t2.csv.\n",
      "Processing Remifasol57\n",
      "Writing 23 tweets in multiusers191213/Remifasol57_mentions_t2.csv.\n",
      "Processing AitiDouze\n",
      "Writing 34 tweets in multiusers191213/AitiDouze_mentions_t2.csv.\n",
      "Processing QAnonAustria1\n",
      "Empty tweet list. Processing stopped for user  QAnonAustria1\n",
      "Processing gotteswerk2411\n",
      "Writing 9 tweets in multiusers191213/gotteswerk2411_mentions_t2.csv.\n",
      "Processing KlimaschutzCH\n",
      "Writing 2 tweets in multiusers191213/KlimaschutzCH_mentions_t2.csv.\n",
      "Processing GrueneCH\n",
      "Writing 1 tweets in multiusers191213/GrueneCH_mentions_t2.csv.\n",
      "Processing proclimCH\n",
      "Writing 1 tweets in multiusers191213/proclimCH_mentions_t2.csv.\n",
      "Processing EperonP\n",
      "Empty tweet list. Processing stopped for user  EperonP\n",
      "Processing MathiasTemujin\n",
      "Empty tweet list. Processing stopped for user  MathiasTemujin\n",
      "Processing klimastreik\n",
      "Empty tweet list. Processing stopped for user  klimastreik\n",
      "Processing AlimEquitables\n",
      "Empty tweet list. Processing stopped for user  AlimEquitables\n",
      "Processing ProNaturaSuisse\n",
      "Empty tweet list. Processing stopped for user  ProNaturaSuisse\n",
      "Processing vertliberaux\n",
      "Empty tweet list. Processing stopped for user  vertliberaux\n",
      "Processing Munsterma\n",
      "Writing 17 tweets in multiusers191213/Munsterma_mentions_t2.csv.\n",
      "Processing bourg_d\n",
      "Writing 14 tweets in multiusers191213/bourg_d_mentions_t2.csv.\n",
      "Processing LesVertsSuisses\n",
      "Empty tweet list. Processing stopped for user  LesVertsSuisses\n",
      "Processing ClimatSuisse\n",
      "Empty tweet list. Processing stopped for user  ClimatSuisse\n",
      "Processing gpsuisse\n",
      "Empty tweet list. Processing stopped for user  gpsuisse\n",
      "Processing IliasPanchard\n",
      "Writing 5 tweets in multiusers191213/IliasPanchard_mentions_t2.csv.\n",
      "Processing ATE_Suisse\n",
      "Empty tweet list. Processing stopped for user  ATE_Suisse\n",
      "\n",
      "******* Processing users at 1-hop distance *******\n",
      "Collecting the tweets for the last 2 days.\n",
      "Processing lilmaouz\n",
      "Writing 11 tweets in multiusers191213/lilmaouz_mentions_t2.csv.\n",
      "Processing RaderSerge\n",
      "Writing 10 tweets in multiusers191213/RaderSerge_mentions_t2.csv.\n",
      "Processing RTSinfo\n",
      "Writing 29 tweets in multiusers191213/RTSinfo_mentions_t2.csv.\n",
      "Processing HarryoFfm\n",
      "Writing 11 tweets in multiusers191213/HarryoFfm_mentions_t2.csv.\n",
      "Processing Courtoisix\n",
      "Writing 34 tweets in multiusers191213/Courtoisix_mentions_t2.csv.\n",
      "Processing EnModeMacaron\n",
      "Writing 2 tweets in multiusers191213/EnModeMacaron_mentions_t2.csv.\n",
      "Processing antoinedroux\n",
      "Empty tweet list. Processing stopped for user  antoinedroux\n",
      "Processing MirkoToppano\n",
      "Writing 2 tweets in multiusers191213/MirkoToppano_mentions_t2.csv.\n",
      "Processing pierrejovanovic\n",
      "Writing 79 tweets in multiusers191213/pierrejovanovic_mentions_t2.csv.\n",
      "Processing medialogues\n",
      "Empty tweet list. Processing stopped for user  medialogues\n",
      "Processing BDoowayst\n",
      "Writing 4 tweets in multiusers191213/BDoowayst_mentions_t2.csv.\n",
      "Processing Studiodjip11\n",
      "Writing 12 tweets in multiusers191213/Studiodjip11_mentions_t2.csv.\n",
      "Processing le_Parisien\n",
      "Writing 31 tweets in multiusers191213/le_Parisien_mentions_t2.csv.\n",
      "\n",
      "******* Processing users at 2-hop distance *******\n",
      "Collecting the tweets for the last 2 days.\n",
      "Processing CgtTuifrance\n",
      "Writing 167 tweets in multiusers191213/CgtTuifrance_mentions_t2.csv.\n",
      "Processing Eintracht\n",
      "Writing 21 tweets in multiusers191213/Eintracht_mentions_t2.csv.\n",
      "Processing Jeuyl\n",
      "Writing 14 tweets in multiusers191213/Jeuyl_mentions_t2.csv.\n",
      "Processing olivierbeaumont\n",
      "Writing 9 tweets in multiusers191213/olivierbeaumont_mentions_t2.csv.\n",
      "Processing 19h30RTS\n",
      "Writing 6 tweets in multiusers191213/19h30RTS_mentions_t2.csv.\n",
      "Processing Forum_RTS\n",
      "Writing 6 tweets in multiusers191213/Forum_RTS_mentions_t2.csv.\n",
      "Processing LLG2022\n",
      "Writing 71 tweets in multiusers191213/LLG2022_mentions_t2.csv.\n",
      "Processing VincentVerier\n",
      "Writing 1 tweets in multiusers191213/VincentVerier_mentions_t2.csv.\n",
      "Processing jeremycorbyn\n",
      "Writing 59 tweets in multiusers191213/jeremycorbyn_mentions_t2.csv.\n",
      "Processing PoliceRealites\n",
      "Empty tweet list. Processing stopped for user  PoliceRealites\n",
      "Processing BorisJohnson\n",
      "Writing 5 tweets in multiusers191213/BorisJohnson_mentions_t2.csv.\n",
      "Processing ChLECHEVALIER\n",
      "Writing 89 tweets in multiusers191213/ChLECHEVALIER_mentions_t2.csv.\n",
      "Processing Valeurs\n",
      "Writing 15 tweets in multiusers191213/Valeurs_mentions_t2.csv.\n",
      "Processing DMagiques\n",
      "Writing 17 tweets in multiusers191213/DMagiques_mentions_t2.csv.\n",
      "Processing larry_GVA\n",
      "Writing 7 tweets in multiusers191213/larry_GVA_mentions_t2.csv.\n",
      "Processing DiciWashington\n",
      "Writing 12 tweets in multiusers191213/DiciWashington_mentions_t2.csv.\n",
      "Processing LeParisienInfog\n",
      "Empty tweet list. Processing stopped for user  LeParisienInfog\n",
      "Processing nicolasberrod\n",
      "Writing 26 tweets in multiusers191213/nicolasberrod_mentions_t2.csv.\n",
      "Processing nenon0607\n",
      "Writing 181 tweets in multiusers191213/nenon0607_mentions_t2.csv.\n",
      "\n",
      "******* Processing users at 3-hop distance *******\n",
      "Collecting the tweets for the last 2 days.\n",
      "Processing angelal33242968\n",
      "Empty tweet list. Processing stopped for user  angelal33242968\n",
      "Processing DominiqueLUNEL\n",
      "Writing 59 tweets in multiusers191213/DominiqueLUNEL_mentions_t2.csv.\n",
      "Processing Lormontov\n",
      "Twitter API returned error 401 for user Lormontov.\n",
      "Empty tweet list. Processing stopped for user  Lormontov\n",
      "Processing Fakir_\n",
      "Writing 59 tweets in multiusers191213/Fakir__mentions_t2.csv.\n",
      "Processing DD75007\n",
      "Writing 31 tweets in multiusers191213/DD75007_mentions_t2.csv.\n",
      "Processing EuropaLeague\n",
      "Writing 22 tweets in multiusers191213/EuropaLeague_mentions_t2.csv.\n",
      "Processing MoniquePlaza3\n",
      "Writing 6 tweets in multiusers191213/MoniquePlaza3_mentions_t2.csv.\n",
      "Processing guardian\n",
      "Empty tweet list. Processing stopped for user  guardian\n",
      "Processing fferaerts\n",
      "Writing 24 tweets in multiusers191213/fferaerts_mentions_t2.csv.\n",
      "Processing eric_guillaumin\n",
      "Writing 6 tweets in multiusers191213/eric_guillaumin_mentions_t2.csv.\n",
      "Processing camusetfrederic\n",
      "Writing 39 tweets in multiusers191213/camusetfrederic_mentions_t2.csv.\n",
      "Processing GuillaumeQ1T1\n",
      "Writing 148 tweets in multiusers191213/GuillaumeQ1T1_mentions_t2.csv.\n",
      "Processing tegnererik\n",
      "Writing 29 tweets in multiusers191213/tegnererik_mentions_t2.csv.\n",
      "Processing patriotedu11\n",
      "Twitter API returned error 401 for user patriotedu11.\n",
      "Empty tweet list. Processing stopped for user  patriotedu11\n",
      "Processing RobertMenardFR\n",
      "Writing 4 tweets in multiusers191213/RobertMenardFR_mentions_t2.csv.\n",
      "Processing Elisabeth92390\n",
      "Writing 107 tweets in multiusers191213/Elisabeth92390_mentions_t2.csv.\n",
      "Processing arretsurimages\n",
      "Writing 8 tweets in multiusers191213/arretsurimages_mentions_t2.csv.\n",
      "Processing patrick_edery\n",
      "Writing 21 tweets in multiusers191213/patrick_edery_mentions_t2.csv.\n",
      "Processing frisson2com\n",
      "Writing 60 tweets in multiusers191213/frisson2com_mentions_t2.csv.\n",
      "Processing BalanceTonPost\n",
      "Writing 13 tweets in multiusers191213/BalanceTonPost_mentions_t2.csv.\n",
      "Processing cresus1ier\n",
      "Writing 167 tweets in multiusers191213/cresus1ier_mentions_t2.csv.\n",
      "Processing HussonAlex\n",
      "Writing 19 tweets in multiusers191213/HussonAlex_mentions_t2.csv.\n",
      "Processing JeanMar35370554\n",
      "Writing 47 tweets in multiusers191213/JeanMar35370554_mentions_t2.csv.\n",
      "Processing Miss_Nerdy88\n",
      "Writing 12 tweets in multiusers191213/Miss_Nerdy88_mentions_t2.csv.\n",
      "Processing RNational_off\n",
      "Writing 19 tweets in multiusers191213/RNational_off_mentions_t2.csv.\n",
      "Processing Portes_Thomas\n",
      "Writing 19 tweets in multiusers191213/Portes_Thomas_mentions_t2.csv.\n",
      "Processing Tsipora777\n",
      "Writing 146 tweets in multiusers191213/Tsipora777_mentions_t2.csv.\n",
      "Processing MauriceMartin01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 131 tweets in multiusers191213/MauriceMartin01_mentions_t2.csv.\n",
      "Processing CCastaner\n",
      "Writing 13 tweets in multiusers191213/CCastaner_mentions_t2.csv.\n",
      "Processing jazzpote\n",
      "Writing 62 tweets in multiusers191213/jazzpote_mentions_t2.csv.\n",
      "Processing cathe6915\n",
      "Writing 100 tweets in multiusers191213/cathe6915_mentions_t2.csv.\n",
      "Processing anticor_org\n",
      "Writing 15 tweets in multiusers191213/anticor_org_mentions_t2.csv.\n",
      "Processing mediaslibres\n",
      "Writing 3 tweets in multiusers191213/mediaslibres_mentions_t2.csv.\n",
      "Processing RachelChouchana\n",
      "Writing 184 tweets in multiusers191213/RachelChouchana_mentions_t2.csv.\n",
      "Processing OrVieilli\n",
      "Twitter API returned error 401 for user OrVieilli.\n",
      "Empty tweet list. Processing stopped for user  OrVieilli\n",
      "Processing Pollueur_\n",
      "Writing 8 tweets in multiusers191213/Pollueur__mentions_t2.csv.\n",
      "Processing SudRailCentraux\n",
      "Writing 130 tweets in multiusers191213/SudRailCentraux_mentions_t2.csv.\n",
      "Processing mimimatic\n",
      "Writing 44 tweets in multiusers191213/mimimatic_mentions_t2.csv.\n",
      "Processing Damocles_Fr\n",
      "Writing 33 tweets in multiusers191213/Damocles_Fr_mentions_t2.csv.\n",
      "Processing PNerval\n",
      "Writing 129 tweets in multiusers191213/PNerval_mentions_t2.csv.\n",
      "Processing FrancoisFillon\n",
      "Empty tweet list. Processing stopped for user  FrancoisFillon\n",
      "Processing jchribuisson\n",
      "Writing 11 tweets in multiusers191213/jchribuisson_mentions_t2.csv.\n",
      "Processing Vertumne1\n",
      "Writing 30 tweets in multiusers191213/Vertumne1_mentions_t2.csv.\n",
      "Processing ataraxie66\n",
      "Writing 139 tweets in multiusers191213/ataraxie66_mentions_t2.csv.\n",
      "Processing elizabethaoust\n",
      "Writing 33 tweets in multiusers191213/elizabethaoust_mentions_t2.csv.\n",
      "Processing LEXPRESS\n",
      "Writing 4 tweets in multiusers191213/LEXPRESS_mentions_t2.csv.\n",
      "Processing Marie_dz\n",
      "Writing 145 tweets in multiusers191213/Marie_dz_mentions_t2.csv.\n",
      "Processing Orhiecsou\n",
      "Writing 21 tweets in multiusers191213/Orhiecsou_mentions_t2.csv.\n",
      "Processing MarionMarechal\n",
      "Writing 1 tweets in multiusers191213/MarionMarechal_mentions_t2.csv.\n",
      "Processing JstLangevin\n",
      "Writing 139 tweets in multiusers191213/JstLangevin_mentions_t2.csv.\n",
      "Processing EmmanuelMacron\n",
      "Writing 1 tweets in multiusers191213/EmmanuelMacron_mentions_t2.csv.\n",
      "Processing sylvita30\n",
      "Writing 157 tweets in multiusers191213/sylvita30_mentions_t2.csv.\n",
      "Processing 20hFrance2\n",
      "Writing 15 tweets in multiusers191213/20hFrance2_mentions_t2.csv.\n",
      "Processing CNEWS\n",
      "Writing 6 tweets in multiusers191213/CNEWS_mentions_t2.csv.\n",
      "Processing adia66\n",
      "Writing 107 tweets in multiusers191213/adia66_mentions_t2.csv.\n",
      "Processing Andr3iNapai0v\n",
      "Twitter API returned error 401 for user Andr3iNapai0v.\n",
      "Empty tweet list. Processing stopped for user  Andr3iNapai0v\n",
      "Processing LysLady1\n",
      "Writing 41 tweets in multiusers191213/LysLady1_mentions_t2.csv.\n",
      "Processing ConvDeLaDroite\n",
      "Writing 14 tweets in multiusers191213/ConvDeLaDroite_mentions_t2.csv.\n",
      "Processing LibDems\n",
      "Writing 41 tweets in multiusers191213/LibDems_mentions_t2.csv.\n",
      "Processing ibalkany\n",
      "Writing 4 tweets in multiusers191213/ibalkany_mentions_t2.csv.\n",
      "Processing ogrenyx\n",
      "Writing 40 tweets in multiusers191213/ogrenyx_mentions_t2.csv.\n",
      "Processing aristogiton3\n",
      "Writing 69 tweets in multiusers191213/aristogiton3_mentions_t2.csv.\n",
      "Processing nathsamson\n",
      "Empty tweet list. Processing stopped for user  nathsamson\n",
      "Processing CFDT\n",
      "Writing 17 tweets in multiusers191213/CFDT_mentions_t2.csv.\n",
      "Processing LarrereMathilde\n",
      "Writing 67 tweets in multiusers191213/LarrereMathilde_mentions_t2.csv.\n",
      "Processing catherinegaste\n",
      "Writing 30 tweets in multiusers191213/catherinegaste_mentions_t2.csv.\n",
      "Processing Montfreu\n",
      "Writing 28 tweets in multiusers191213/Montfreu_mentions_t2.csv.\n",
      "Processing MarianneleMag\n",
      "Writing 5 tweets in multiusers191213/MarianneleMag_mentions_t2.csv.\n",
      "Processing Sardoche_Lol\n",
      "Writing 47 tweets in multiusers191213/Sardoche_Lol_mentions_t2.csv.\n",
      "Processing lesRepublicains\n",
      "Writing 39 tweets in multiusers191213/lesRepublicains_mentions_t2.csv.\n",
      "Processing PietroBugnon\n",
      "Empty tweet list. Processing stopped for user  PietroBugnon\n",
      "Processing erichacquemand\n",
      "Writing 20 tweets in multiusers191213/erichacquemand_mentions_t2.csv.\n",
      "Processing Conservatives\n",
      "Writing 57 tweets in multiusers191213/Conservatives_mentions_t2.csv.\n",
      "Processing EPhilippePM\n",
      "Writing 2 tweets in multiusers191213/EPhilippePM_mentions_t2.csv.\n",
      "Processing Gato_Monteee\n",
      "Twitter API returned error 401 for user Gato_Monteee.\n",
      "Empty tweet list. Processing stopped for user  Gato_Monteee\n",
      "Processing BFMTV\n",
      "Writing 13 tweets in multiusers191213/BFMTV_mentions_t2.csv.\n",
      "Processing UKLabour\n",
      "Writing 60 tweets in multiusers191213/UKLabour_mentions_t2.csv.\n",
      "Processing NBelloubet\n",
      "Writing 5 tweets in multiusers191213/NBelloubet_mentions_t2.csv.\n",
      "Processing JLMelenchon\n",
      "Writing 1 tweets in multiusers191213/JLMelenchon_mentions_t2.csv.\n",
      "Processing Napo1852\n",
      "Writing 25 tweets in multiusers191213/Napo1852_mentions_t2.csv.\n",
      "Processing bea9214\n",
      "Writing 7 tweets in multiusers191213/bea9214_mentions_t2.csv.\n",
      "Processing bedfordfoodbank\n",
      "Empty tweet list. Processing stopped for user  bedfordfoodbank\n",
      "Processing Alvisyma\n",
      "Writing 127 tweets in multiusers191213/Alvisyma_mentions_t2.csv.\n",
      "Processing CSpontanees\n",
      "Writing 175 tweets in multiusers191213/CSpontanees_mentions_t2.csv.\n",
      "Processing MLP_officiel\n",
      "Writing 7 tweets in multiusers191213/MLP_officiel_mentions_t2.csv.\n",
      "Processing enmarchefr\n",
      "Writing 9 tweets in multiusers191213/enmarchefr_mentions_t2.csv.\n",
      "Processing kkerima\n",
      "Writing 39 tweets in multiusers191213/kkerima_mentions_t2.csv.\n",
      "Processing __Verlaine__\n",
      "Writing 67 tweets in multiusers191213/__Verlaine___mentions_t2.csv.\n",
      "Processing eintracht_lz\n",
      "Writing 1 tweets in multiusers191213/eintracht_lz_mentions_t2.csv.\n",
      "Processing LGBTQillyria\n",
      "Writing 56 tweets in multiusers191213/LGBTQillyria_mentions_t2.csv.\n",
      "\n",
      "******* Processing users at 4-hop distance *******\n",
      "Collecting the tweets for the last 2 days.\n",
      "Processing CfdtBerger\n",
      "Writing 6 tweets in multiusers191213/CfdtBerger_mentions_t2.csv.\n",
      "Processing TeamProgressist\n",
      "Writing 6 tweets in multiusers191213/TeamProgressist_mentions_t2.csv.\n",
      "Processing MatthieuGariel\n",
      "Writing 129 tweets in multiusers191213/MatthieuGariel_mentions_t2.csv.\n",
      "Processing fleurdepee\n",
      "Writing 132 tweets in multiusers191213/fleurdepee_mentions_t2.csv.\n",
      "Processing christine_kelly\n",
      "Writing 14 tweets in multiusers191213/christine_kelly_mentions_t2.csv.\n",
      "Processing KateRoc13\n",
      "Writing 45 tweets in multiusers191213/KateRoc13_mentions_t2.csv.\n",
      "Processing AlexDevecchio\n",
      "Writing 11 tweets in multiusers191213/AlexDevecchio_mentions_t2.csv.\n",
      "Processing saccomno\n",
      "Empty tweet list. Processing stopped for user  saccomno\n",
      "Processing Mariett64686240\n",
      "Writing 49 tweets in multiusers191213/Mariett64686240_mentions_t2.csv.\n",
      "Processing Marinalouloutte\n",
      "Writing 25 tweets in multiusers191213/Marinalouloutte_mentions_t2.csv.\n",
      "Processing Henri_B1011\n",
      "Writing 61 tweets in multiusers191213/Henri_B1011_mentions_t2.csv.\n",
      "Processing GaveEmmanuelle\n",
      "Writing 30 tweets in multiusers191213/GaveEmmanuelle_mentions_t2.csv.\n",
      "Processing leblanc_hans\n",
      "Writing 105 tweets in multiusers191213/leblanc_hans_mentions_t2.csv.\n",
      "Processing EmmWargon\n",
      "Writing 1 tweets in multiusers191213/EmmWargon_mentions_t2.csv.\n",
      "Processing LeParisien_75\n",
      "Writing 2 tweets in multiusers191213/LeParisien_75_mentions_t2.csv.\n",
      "Processing FRpropagandan\n",
      "Writing 35 tweets in multiusers191213/FRpropagandan_mentions_t2.csv.\n",
      "Processing damienabad\n",
      "Writing 5 tweets in multiusers191213/damienabad_mentions_t2.csv.\n",
      "Processing beatricelecoz\n",
      "Writing 54 tweets in multiusers191213/beatricelecoz_mentions_t2.csv.\n",
      "Processing kowakunai_kyo\n",
      "Writing 122 tweets in multiusers191213/kowakunai_kyo_mentions_t2.csv.\n",
      "Processing Marieambre22\n",
      "Writing 29 tweets in multiusers191213/Marieambre22_mentions_t2.csv.\n",
      "Processing fichesducinema\n",
      "Empty tweet list. Processing stopped for user  fichesducinema\n",
      "Processing hankookreifen\n",
      "Empty tweet list. Processing stopped for user  hankookreifen\n",
      "Processing AmezianeRose\n",
      "Writing 4 tweets in multiusers191213/AmezianeRose_mentions_t2.csv.\n",
      "Processing x_xdir\n",
      "Writing 166 tweets in multiusers191213/x_xdir_mentions_t2.csv.\n",
      "Processing J_Bardella\n",
      "Writing 19 tweets in multiusers191213/J_Bardella_mentions_t2.csv.\n",
      "Processing Manifou\n",
      "Writing 123 tweets in multiusers191213/Manifou_mentions_t2.csv.\n",
      "Processing monnier403\n",
      "Writing 72 tweets in multiusers191213/monnier403_mentions_t2.csv.\n",
      "Processing Sakura_MXCIX\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter API returned error 401 for user Sakura_MXCIX.\n",
      "Empty tweet list. Processing stopped for user  Sakura_MXCIX\n",
      "Processing gillesjohnson\n",
      "Writing 57 tweets in multiusers191213/gillesjohnson_mentions_t2.csv.\n",
      "Processing Triple_Donation\n",
      "Writing 71 tweets in multiusers191213/Triple_Donation_mentions_t2.csv.\n",
      "Processing mimig1953\n",
      "Writing 27 tweets in multiusers191213/mimig1953_mentions_t2.csv.\n",
      "Processing ThierryMARIANI\n",
      "Writing 5 tweets in multiusers191213/ThierryMARIANI_mentions_t2.csv.\n",
      "Processing PierreMenes\n",
      "Writing 23 tweets in multiusers191213/PierreMenes_mentions_t2.csv.\n",
      "Processing Amandine9782\n",
      "Writing 55 tweets in multiusers191213/Amandine9782_mentions_t2.csv.\n",
      "Processing LibreRegis\n",
      "Writing 184 tweets in multiusers191213/LibreRegis_mentions_t2.csv.\n",
      "Processing flouppi\n",
      "Writing 81 tweets in multiusers191213/flouppi_mentions_t2.csv.\n",
      "Processing AvecLisnard\n",
      "Writing 8 tweets in multiusers191213/AvecLisnard_mentions_t2.csv.\n",
      "Processing CgtRatp\n",
      "Writing 4 tweets in multiusers191213/CgtRatp_mentions_t2.csv.\n",
      "Processing JeanMessiha\n",
      "Writing 22 tweets in multiusers191213/JeanMessiha_mentions_t2.csv.\n",
      "Processing DesobeauxLaeti1\n",
      "Writing 76 tweets in multiusers191213/DesobeauxLaeti1_mentions_t2.csv.\n",
      "Processing LucioleB5\n",
      "Writing 60 tweets in multiusers191213/LucioleB5_mentions_t2.csv.\n",
      "Processing cap_ou_pacap\n",
      "Writing 8 tweets in multiusers191213/cap_ou_pacap_mentions_t2.csv.\n",
      "Processing _Stalker_69_\n",
      "Writing 145 tweets in multiusers191213/_Stalker_69__mentions_t2.csv.\n",
      "Processing ManUtd\n",
      "Writing 15 tweets in multiusers191213/ManUtd_mentions_t2.csv.\n",
      "Processing Mnnbv13355811\n",
      "Writing 90 tweets in multiusers191213/Mnnbv13355811_mentions_t2.csv.\n",
      "Processing dupontaignan\n",
      "Writing 3 tweets in multiusers191213/dupontaignan_mentions_t2.csv.\n",
      "Processing LIGNER_SNCF\n",
      "Writing 67 tweets in multiusers191213/LIGNER_SNCF_mentions_t2.csv.\n",
      "Processing LeTelegramme\n",
      "Writing 40 tweets in multiusers191213/LeTelegramme_mentions_t2.csv.\n",
      "Processing Samuel_Lafont\n",
      "Writing 34 tweets in multiusers191213/Samuel_Lafont_mentions_t2.csv.\n",
      "Processing GrandPalaisRmn\n",
      "Writing 8 tweets in multiusers191213/GrandPalaisRmn_mentions_t2.csv.\n",
      "Processing Brindmine1708\n",
      "Writing 70 tweets in multiusers191213/Brindmine1708_mentions_t2.csv.\n",
      "Processing IamKVNPCM\n",
      "Writing 17 tweets in multiusers191213/IamKVNPCM_mentions_t2.csv.\n",
      "Processing FascistFemboy\n",
      "Writing 70 tweets in multiusers191213/FascistFemboy_mentions_t2.csv.\n",
      "Processing Auvergnat_Deter\n",
      "Writing 6 tweets in multiusers191213/Auvergnat_Deter_mentions_t2.csv.\n",
      "Processing watching_giants\n",
      "Writing 10 tweets in multiusers191213/watching_giants_mentions_t2.csv.\n",
      "Processing MalekDelegue\n",
      "Writing 111 tweets in multiusers191213/MalekDelegue_mentions_t2.csv.\n",
      "Processing AM_Alumni\n",
      "Empty tweet list. Processing stopped for user  AM_Alumni\n",
      "Processing Elsa_Gambin\n",
      "Writing 31 tweets in multiusers191213/Elsa_Gambin_mentions_t2.csv.\n",
      "Processing Hgibier\n",
      "Writing 51 tweets in multiusers191213/Hgibier_mentions_t2.csv.\n",
      "Processing Puck_Fair_\n",
      "Writing 158 tweets in multiusers191213/Puck_Fair__mentions_t2.csv.\n",
      "Processing pascal5123\n",
      "Writing 47 tweets in multiusers191213/pascal5123_mentions_t2.csv.\n",
      "Processing kreme2kassis\n",
      "Writing 105 tweets in multiusers191213/kreme2kassis_mentions_t2.csv.\n",
      "Processing GaccioB\n",
      "Writing 41 tweets in multiusers191213/GaccioB_mentions_t2.csv.\n",
      "Processing carriesymonds\n",
      "Writing 10 tweets in multiusers191213/carriesymonds_mentions_t2.csv.\n",
      "Processing gregoryroose\n",
      "Writing 28 tweets in multiusers191213/gregoryroose_mentions_t2.csv.\n",
      "Processing TEAMPSGULTRA\n",
      "Writing 6 tweets in multiusers191213/TEAMPSGULTRA_mentions_t2.csv.\n",
      "Processing Louisedescourt\n",
      "Writing 94 tweets in multiusers191213/Louisedescourt_mentions_t2.csv.\n",
      "Processing ECiotti\n",
      "Writing 9 tweets in multiusers191213/ECiotti_mentions_t2.csv.\n",
      "Processing Hariulfi_Curtis\n",
      "Writing 38 tweets in multiusers191213/Hariulfi_Curtis_mentions_t2.csv.\n",
      "Processing justice_gouv\n",
      "Writing 4 tweets in multiusers191213/justice_gouv_mentions_t2.csv.\n",
      "Processing GillesBornstein\n",
      "Empty tweet list. Processing stopped for user  GillesBornstein\n",
      "Processing caro_ligne\n",
      "Writing 12 tweets in multiusers191213/caro_ligne_mentions_t2.csv.\n",
      "Processing Francois_Ruffin\n",
      "Writing 2 tweets in multiusers191213/Francois_Ruffin_mentions_t2.csv.\n",
      "Processing SandraBks\n",
      "Writing 45 tweets in multiusers191213/SandraBks_mentions_t2.csv.\n",
      "Processing GPirouli\n",
      "Writing 92 tweets in multiusers191213/GPirouli_mentions_t2.csv.\n",
      "Processing laurencedecock1\n",
      "Writing 29 tweets in multiusers191213/laurencedecock1_mentions_t2.csv.\n",
      "Processing PageBalkany\n",
      "Writing 13 tweets in multiusers191213/PageBalkany_mentions_t2.csv.\n",
      "Processing girlssmellweird\n",
      "Writing 85 tweets in multiusers191213/girlssmellweird_mentions_t2.csv.\n",
      "Processing llizygreen\n",
      "Writing 36 tweets in multiusers191213/llizygreen_mentions_t2.csv.\n",
      "Processing melonland2017\n",
      "Writing 82 tweets in multiusers191213/melonland2017_mentions_t2.csv.\n",
      "Processing apocalypto06\n",
      "Writing 45 tweets in multiusers191213/apocalypto06_mentions_t2.csv.\n",
      "Processing florencehenry56\n",
      "Writing 38 tweets in multiusers191213/florencehenry56_mentions_t2.csv.\n",
      "Processing Dr_moji\n",
      "Writing 14 tweets in multiusers191213/Dr_moji_mentions_t2.csv.\n",
      "Processing orphee45\n",
      "Writing 93 tweets in multiusers191213/orphee45_mentions_t2.csv.\n",
      "Processing PatrickRogerE\n",
      "Writing 9 tweets in multiusers191213/PatrickRogerE_mentions_t2.csv.\n",
      "Processing tombeaucombat\n",
      "Writing 113 tweets in multiusers191213/tombeaucombat_mentions_t2.csv.\n",
      "Processing sarroche\n",
      "Writing 4 tweets in multiusers191213/sarroche_mentions_t2.csv.\n",
      "Processing solebesne\n",
      "Writing 161 tweets in multiusers191213/solebesne_mentions_t2.csv.\n",
      "Processing F_Desouche\n",
      "Writing 1 tweets in multiusers191213/F_Desouche_mentions_t2.csv.\n",
      "Processing HeraklesSoter\n",
      "Writing 63 tweets in multiusers191213/HeraklesSoter_mentions_t2.csv.\n",
      "Processing le_gorafi\n",
      "Writing 4 tweets in multiusers191213/le_gorafi_mentions_t2.csv.\n",
      "Processing GWGoldnadel\n",
      "Writing 13 tweets in multiusers191213/GWGoldnadel_mentions_t2.csv.\n",
      "Processing RobertJenrick\n",
      "Writing 8 tweets in multiusers191213/RobertJenrick_mentions_t2.csv.\n",
      "Processing CNEWS_Sport\n",
      "Writing 3 tweets in multiusers191213/CNEWS_Sport_mentions_t2.csv.\n",
      "Processing Remuverang1\n",
      "Writing 111 tweets in multiusers191213/Remuverang1_mentions_t2.csv.\n",
      "Processing C8TV\n",
      "Writing 72 tweets in multiusers191213/C8TV_mentions_t2.csv.\n",
      "Processing frenchkaiser\n",
      "Writing 29 tweets in multiusers191213/frenchkaiser_mentions_t2.csv.\n",
      "Processing AARHOIODlAN\n",
      "Writing 51 tweets in multiusers191213/AARHOIODlAN_mentions_t2.csv.\n",
      "Processing gouvernementFR\n",
      "Writing 9 tweets in multiusers191213/gouvernementFR_mentions_t2.csv.\n",
      "Processing elisahk92\n",
      "Writing 87 tweets in multiusers191213/elisahk92_mentions_t2.csv.\n",
      "Processing DERIEDMATTEN1\n",
      "Empty tweet list. Processing stopped for user  DERIEDMATTEN1\n",
      "Processing GebekaFilms\n",
      "Writing 2 tweets in multiusers191213/GebekaFilms_mentions_t2.csv.\n",
      "Processing cecile2menibus\n",
      "Writing 4 tweets in multiusers191213/cecile2menibus_mentions_t2.csv.\n",
      "Processing Franck82170511\n",
      "Writing 49 tweets in multiusers191213/Franck82170511_mentions_t2.csv.\n",
      "Processing EmmacruelMacron\n",
      "Writing 102 tweets in multiusers191213/EmmacruelMacron_mentions_t2.csv.\n",
      "Processing KhaledFreak\n",
      "Writing 3 tweets in multiusers191213/KhaledFreak_mentions_t2.csv.\n",
      "Processing FrancineCosimi1\n",
      "Writing 3 tweets in multiusers191213/FrancineCosimi1_mentions_t2.csv.\n",
      "Processing rakauto\n",
      "Writing 10 tweets in multiusers191213/rakauto_mentions_t2.csv.\n",
      "Processing aurelien_veron\n",
      "Writing 54 tweets in multiusers191213/aurelien_veron_mentions_t2.csv.\n",
      "Processing RTenfrancais\n",
      "Writing 43 tweets in multiusers191213/RTenfrancais_mentions_t2.csv.\n",
      "Processing LobotomizedTrap\n",
      "Twitter API returned error 401 for user LobotomizedTrap.\n",
      "Empty tweet list. Processing stopped for user  LobotomizedTrap\n",
      "Processing KelfaJ\n",
      "Writing 16 tweets in multiusers191213/KelfaJ_mentions_t2.csv.\n",
      "Processing SaphirElle83\n",
      "Writing 30 tweets in multiusers191213/SaphirElle83_mentions_t2.csv.\n",
      "Processing RaquelGarridoFr\n",
      "Writing 22 tweets in multiusers191213/RaquelGarridoFr_mentions_t2.csv.\n",
      "Processing NPolony\n",
      "Writing 6 tweets in multiusers191213/NPolony_mentions_t2.csv.\n",
      "Processing joswinson\n",
      "Writing 6 tweets in multiusers191213/joswinson_mentions_t2.csv.\n",
      "Processing francetvchine\n",
      "Writing 1 tweets in multiusers191213/francetvchine_mentions_t2.csv.\n",
      "Processing DelarueJC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 46 tweets in multiusers191213/DelarueJC_mentions_t2.csv.\n",
      "Processing DeSpartacus\n",
      "Writing 111 tweets in multiusers191213/DeSpartacus_mentions_t2.csv.\n",
      "Processing Jade_Bekhti\n",
      "Writing 102 tweets in multiusers191213/Jade_Bekhti_mentions_t2.csv.\n",
      "Processing YTCreateurs\n",
      "Writing 15 tweets in multiusers191213/YTCreateurs_mentions_t2.csv.\n",
      "Processing MargSchinas\n",
      "Writing 9 tweets in multiusers191213/MargSchinas_mentions_t2.csv.\n",
      "Processing YlvaJohansson\n",
      "Writing 13 tweets in multiusers191213/YlvaJohansson_mentions_t2.csv.\n",
      "Processing CouraudMaya\n",
      "Writing 211 tweets in multiusers191213/CouraudMaya_mentions_t2.csv.\n",
      "Processing MaryCherby\n",
      "Writing 172 tweets in multiusers191213/MaryCherby_mentions_t2.csv.\n",
      "Processing LauraPidcockMP\n",
      "Writing 33 tweets in multiusers191213/LauraPidcockMP_mentions_t2.csv.\n",
      "Processing natechonchon\n",
      "Writing 150 tweets in multiusers191213/natechonchon_mentions_t2.csv.\n",
      "Processing RMCinfo\n",
      "Writing 29 tweets in multiusers191213/RMCinfo_mentions_t2.csv.\n",
      "Processing ZemmourFaceInfo\n",
      "Writing 5 tweets in multiusers191213/ZemmourFaceInfo_mentions_t2.csv.\n",
      "Processing Abbe_Rezina\n",
      "Writing 102 tweets in multiusers191213/Abbe_Rezina_mentions_t2.csv.\n",
      "Processing Alex451618\n",
      "Writing 68 tweets in multiusers191213/Alex451618_mentions_t2.csv.\n",
      "Processing alexandrecalvez\n",
      "Writing 3 tweets in multiusers191213/alexandrecalvez_mentions_t2.csv.\n",
      "Processing Neurotherapeute\n",
      "Writing 1 tweets in multiusers191213/Neurotherapeute_mentions_t2.csv.\n",
      "Processing ESJLille\n",
      "Writing 5 tweets in multiusers191213/ESJLille_mentions_t2.csv.\n",
      "Processing EmRiposte\n",
      "Writing 41 tweets in multiusers191213/EmRiposte_mentions_t2.csv.\n",
      "Processing GBOU66\n",
      "Writing 86 tweets in multiusers191213/GBOU66_mentions_t2.csv.\n",
      "Processing realDonaldTrump\n",
      "Writing 1 tweets in multiusers191213/realDonaldTrump_mentions_t2.csv.\n",
      "Processing Bassinrebel\n",
      "Writing 181 tweets in multiusers191213/Bassinrebel_mentions_t2.csv.\n",
      "Processing davidlisnard\n",
      "Writing 53 tweets in multiusers191213/davidlisnard_mentions_t2.csv.\n",
      "Processing cfdtcheminots\n",
      "Writing 34 tweets in multiusers191213/cfdtcheminots_mentions_t2.csv.\n",
      "Processing dymoney77\n",
      "Writing 51 tweets in multiusers191213/dymoney77_mentions_t2.csv.\n",
      "Processing Place_Beauvau\n",
      "Writing 18 tweets in multiusers191213/Place_Beauvau_mentions_t2.csv.\n",
      "Processing MarchaisTh\n",
      "Writing 63 tweets in multiusers191213/MarchaisTh_mentions_t2.csv.\n",
      "Processing DidierMaisto\n",
      "Writing 73 tweets in multiusers191213/DidierMaisto_mentions_t2.csv.\n",
      "Processing zen31083632\n",
      "Writing 80 tweets in multiusers191213/zen31083632_mentions_t2.csv.\n",
      "Processing SansCompteFixe\n",
      "Writing 42 tweets in multiusers191213/SansCompteFixe_mentions_t2.csv.\n",
      "Processing Beccah2Fois\n",
      "Writing 167 tweets in multiusers191213/Beccah2Fois_mentions_t2.csv.\n",
      "Processing LSoubranne\n",
      "Writing 101 tweets in multiusers191213/LSoubranne_mentions_t2.csv.\n",
      "Processing nico2312\n",
      "Writing 17 tweets in multiusers191213/nico2312_mentions_t2.csv.\n",
      "Processing TEYRAS1\n",
      "Writing 28 tweets in multiusers191213/TEYRAS1_mentions_t2.csv.\n",
      "Processing nytimes\n",
      "Writing 45 tweets in multiusers191213/nytimes_mentions_t2.csv.\n",
      "Processing umadrab\n",
      "Twitter API returned error 401 for user umadrab.\n",
      "Empty tweet list. Processing stopped for user  umadrab\n",
      "Processing _MasonGreenwood\n",
      "Writing 6 tweets in multiusers191213/_MasonGreenwood_mentions_t2.csv.\n",
      "Processing TradMethDealer\n",
      "Writing 39 tweets in multiusers191213/TradMethDealer_mentions_t2.csv.\n",
      "Processing ThFerrier\n",
      "Writing 51 tweets in multiusers191213/ThFerrier_mentions_t2.csv.\n",
      "Processing eridan626\n",
      "Writing 55 tweets in multiusers191213/eridan626_mentions_t2.csv.\n",
      "Processing JeanPourtau\n",
      "Writing 22 tweets in multiusers191213/JeanPourtau_mentions_t2.csv.\n",
      "Processing Annelie01190\n",
      "Writing 142 tweets in multiusers191213/Annelie01190_mentions_t2.csv.\n",
      "Processing AliveAndHereNow\n",
      "Writing 59 tweets in multiusers191213/AliveAndHereNow_mentions_t2.csv.\n",
      "Processing MarcMenant1\n",
      "Empty tweet list. Processing stopped for user  MarcMenant1\n",
      "Processing claudarnaud\n",
      "Writing 1 tweets in multiusers191213/claudarnaud_mentions_t2.csv.\n",
      "Processing Blondie2714\n",
      "Writing 7 tweets in multiusers191213/Blondie2714_mentions_t2.csv.\n",
      "Processing Triboque\n",
      "Writing 17 tweets in multiusers191213/Triboque_mentions_t2.csv.\n",
      "Processing fredileparigo\n",
      "Writing 30 tweets in multiusers191213/fredileparigo_mentions_t2.csv.\n",
      "Processing francefirst2\n",
      "Writing 100 tweets in multiusers191213/francefirst2_mentions_t2.csv.\n",
      "Processing Vincent_Grrrr\n",
      "Writing 86 tweets in multiusers191213/Vincent_Grrrr_mentions_t2.csv.\n",
      "Processing achabus\n",
      "Writing 158 tweets in multiusers191213/achabus_mentions_t2.csv.\n",
      "Processing Zezette291\n",
      "Writing 54 tweets in multiusers191213/Zezette291_mentions_t2.csv.\n",
      "Processing motolove66\n",
      "Writing 95 tweets in multiusers191213/motolove66_mentions_t2.csv.\n",
      "Processing Francoiszero2\n",
      "Writing 31 tweets in multiusers191213/Francoiszero2_mentions_t2.csv.\n",
      "Processing mbockcote\n",
      "Writing 1 tweets in multiusers191213/mbockcote_mentions_t2.csv.\n",
      "Processing lcensur\n",
      "Writing 122 tweets in multiusers191213/lcensur_mentions_t2.csv.\n",
      "Processing Valenti80776287\n",
      "Writing 35 tweets in multiusers191213/Valenti80776287_mentions_t2.csv.\n",
      "Processing Senat\n",
      "Writing 32 tweets in multiusers191213/Senat_mentions_t2.csv.\n",
      "Processing grognard_d\n",
      "Writing 66 tweets in multiusers191213/grognard_d_mentions_t2.csv.\n",
      "Processing passionsdelisah\n",
      "Writing 125 tweets in multiusers191213/passionsdelisah_mentions_t2.csv.\n",
      "Processing MdAssilA\n",
      "Writing 12 tweets in multiusers191213/MdAssilA_mentions_t2.csv.\n",
      "Processing france84659339\n",
      "Twitter API returned error 401 for user france84659339.\n",
      "Empty tweet list. Processing stopped for user  france84659339\n",
      "Processing VIGI_MI\n",
      "Writing 20 tweets in multiusers191213/VIGI_MI_mentions_t2.csv.\n",
      "Processing PL1111\n",
      "Writing 38 tweets in multiusers191213/PL1111_mentions_t2.csv.\n",
      "Processing LegrandPersoml\n",
      "Writing 84 tweets in multiusers191213/LegrandPersoml_mentions_t2.csv.\n",
      "Processing davidfayon\n",
      "Writing 7 tweets in multiusers191213/davidfayon_mentions_t2.csv.\n",
      "Processing zebodag\n",
      "Writing 90 tweets in multiusers191213/zebodag_mentions_t2.csv.\n",
      "Processing Breath_of_Arda\n",
      "Empty tweet list. Processing stopped for user  Breath_of_Arda\n",
      "Processing CentristeModere\n",
      "Writing 54 tweets in multiusers191213/CentristeModere_mentions_t2.csv.\n",
      "Processing leJDD\n",
      "Writing 1 tweets in multiusers191213/leJDD_mentions_t2.csv.\n",
      "Processing Mediapart\n",
      "Writing 28 tweets in multiusers191213/Mediapart_mentions_t2.csv.\n",
      "Processing HarryCo91\n",
      "Writing 15 tweets in multiusers191213/HarryCo91_mentions_t2.csv.\n",
      "Processing BFMParis\n",
      "Writing 15 tweets in multiusers191213/BFMParis_mentions_t2.csv.\n",
      "Processing MrSmaaashy420\n",
      "Writing 62 tweets in multiusers191213/MrSmaaashy420_mentions_t2.csv.\n",
      "Processing scania2235\n",
      "Writing 6 tweets in multiusers191213/scania2235_mentions_t2.csv.\n",
      "Processing lise__marie\n",
      "Writing 84 tweets in multiusers191213/lise__marie_mentions_t2.csv.\n",
      "Processing AnasseKazib\n",
      "Writing 20 tweets in multiusers191213/AnasseKazib_mentions_t2.csv.\n",
      "Processing CousineauSamuel\n",
      "Writing 34 tweets in multiusers191213/CousineauSamuel_mentions_t2.csv.\n",
      "Processing slappy_w\n",
      "Writing 15 tweets in multiusers191213/slappy_w_mentions_t2.csv.\n",
      "Processing tropical_boy\n",
      "Writing 139 tweets in multiusers191213/tropical_boy_mentions_t2.csv.\n",
      "Processing PenelopeB\n",
      "Writing 36 tweets in multiusers191213/PenelopeB_mentions_t2.csv.\n",
      "Processing _LePetitCaporal\n",
      "Writing 51 tweets in multiusers191213/_LePetitCaporal_mentions_t2.csv.\n",
      "Processing Djebbari_JB\n",
      "Writing 1 tweets in multiusers191213/Djebbari_JB_mentions_t2.csv.\n",
      "Processing A22117890\n",
      "Writing 49 tweets in multiusers191213/A22117890_mentions_t2.csv.\n",
      "Processing ZeratoRSC2\n",
      "Writing 16 tweets in multiusers191213/ZeratoRSC2_mentions_t2.csv.\n",
      "Processing LydiaGuirous\n",
      "Writing 8 tweets in multiusers191213/LydiaGuirous_mentions_t2.csv.\n",
      "Processing ArielNana26\n",
      "Writing 48 tweets in multiusers191213/ArielNana26_mentions_t2.csv.\n",
      "Processing AlexndraBorchio\n",
      "Writing 10 tweets in multiusers191213/AlexndraBorchio_mentions_t2.csv.\n",
      "Processing Halya_Iriss\n",
      "Writing 17 tweets in multiusers191213/Halya_Iriss_mentions_t2.csv.\n",
      "Processing anapunthanomai\n",
      "Writing 9 tweets in multiusers191213/anapunthanomai_mentions_t2.csv.\n",
      "Processing StephaneAlbouy\n",
      "Writing 4 tweets in multiusers191213/StephaneAlbouy_mentions_t2.csv.\n",
      "Processing Meridiana2008\n",
      "Writing 28 tweets in multiusers191213/Meridiana2008_mentions_t2.csv.\n",
      "Processing RetroNewsFr\n"
     ]
    },
    {
     "ename": "TwythonError",
     "evalue": "Twitter API returned a 503 (Service Unavailable), Over capacity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTwythonError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c8b91ef646ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'******* Processing users at {}-hop distance *******'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_users_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_user_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musername_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_day_old\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_day_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#New users to collect:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0musername_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_users_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_username_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# remove the one already collected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-b04c78dc948a>\u001b[0m in \u001b[0;36mprocess_user_list\u001b[0;34m(python_tweets, data_path, username, thres, max_day_old)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnew_users_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musername_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_user_edgelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_day_old\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_day_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmentions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0musers_mentioned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mention'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8b7b8ec98d2f>\u001b[0m in \u001b[0;36mcreate_user_edgelist\u001b[0;34m(python_tweets, data_path, username, thres, max_day_old)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmention_grouped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_user_mention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpython_tweets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_day_old\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_day_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m#except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#    print('exception catched on user {} !!!!!!!!!!!!'.format(username))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Research/sad/sad_tweets/pysad.py\u001b[0m in \u001b[0;36mcollect_user_mention\u001b[0;34m(username, python_tweets, data_path, max_day_old)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# Return the mentions of a users from its tweets, together with the hashtags of the tweet where the mention is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m#print('Collect the Tweets of the last {} days.'.format(max_day_old))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtweets_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_user_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_tweets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_day_old\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_day_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtweets_dic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'User {} has an empty tweet list.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Research/sad/sad_tweets/pysad.py\u001b[0m in \u001b[0;36mget_user_tweets\u001b[0;34m(tweet_handle, username, count, max_day_old)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtweets_dic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \tfor raw_tweet in tweet_handle.get_user_timeline(screen_name = username,  \n\u001b[0;32m---> 35\u001b[0;31m \t\t\t\t\t\t\t\t\t\t   count = count, include_rts = True, tweet_mode='extended'):\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# Meta data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mtime_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'%a %b %d %H:%M:%S +0000 %Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twython/endpoints.py\u001b[0m in \u001b[0;36mget_user_timeline\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'statuses/user_timeline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mget_user_timeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twython/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, endpoint, params, version)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;34m\"\"\"Shortcut for GET requests via :class:`request`\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twython/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, endpoint, method, params, version)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         content = self._request(url, method=method, params=params,\n\u001b[0;32m--> 264\u001b[0;31m                                 api_call=url)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/twython/api.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, url, method, params, api_call)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 retry_after=response.headers.get('X-Rate-Limit-Reset'))\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTwythonError\u001b[0m: Twitter API returned a 503 (Service Unavailable), Over capacity"
     ]
    }
   ],
   "source": [
    "users_dic = {'username':[], 'Nb_mentions': [], 'mentions_of_mentions': []}\n",
    "print('Collecting the tweets for the last {} days.'.format(max_day_old))\n",
    "exploration_depth = 4\n",
    "total_username_list = username_list\n",
    "for depth in range(exploration_depth):\n",
    "    print('')\n",
    "    print('******* Processing users at {}-hop distance *******'.format(depth))\n",
    "    new_users_list,users_df = process_user_list(python_tweets, data_path, username_list, thres=thres, max_day_old=max_day_old)\n",
    "    #New users to collect:\n",
    "    username_list = list(set(new_users_list).difference(set(total_username_list))) # remove the one already collected\n",
    "    total_username_list += username_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users collected:\n",
      "623 623\n"
     ]
    }
   ],
   "source": [
    "print('Total number of users collected:')\n",
    "print(len(total_username_list),len(set(total_username_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved data into an edge table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiusers191213/Elisabeth92390_mentions_t2.csv with 107 tweets.\n",
      "multiusers191213/mbockcote_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/FascistFemboy_mentions_t2.csv with 70 tweets.\n",
      "multiusers191213/MrSmaaashy420_mentions_t2.csv with 62 tweets.\n",
      "multiusers191213/LLG2022_mentions_t2.csv with 71 tweets.\n",
      "multiusers191213/bea9214_mentions_t2.csv with 7 tweets.\n",
      "multiusers191213/justice_gouv_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/Abbe_Rezina_mentions_t2.csv with 102 tweets.\n",
      "multiusers191213/orphee45_mentions_t2.csv with 93 tweets.\n",
      "multiusers191213/JeanPourtau_mentions_t2.csv with 22 tweets.\n",
      "multiusers191213/ataraxie66_mentions_t2.csv with 139 tweets.\n",
      "multiusers191213/PL1111_mentions_t2.csv with 38 tweets.\n",
      "multiusers191213/Elsa_Gambin_mentions_t2.csv with 31 tweets.\n",
      "multiusers191213/AlexDevecchio_mentions_t2.csv with 11 tweets.\n",
      "multiusers191213/x_xdir_mentions_t2.csv with 166 tweets.\n",
      "multiusers191213/cathe6915_mentions_t2.csv with 100 tweets.\n",
      "multiusers191213/lcensur_mentions_t2.csv with 122 tweets.\n",
      "multiusers191213/BalanceTonPost_mentions_t2.csv with 13 tweets.\n",
      "multiusers191213/proclimCH_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/LarrereMathilde_mentions_t2.csv with 67 tweets.\n",
      "multiusers191213/leblanc_hans_mentions_t2.csv with 105 tweets.\n",
      "multiusers191213/LydiaGuirous_mentions_t2.csv with 8 tweets.\n",
      "multiusers191213/DD75007_mentions_t2.csv with 31 tweets.\n",
      "multiusers191213/SaphirElle83_mentions_t2.csv with 30 tweets.\n",
      "multiusers191213/Halya_Iriss_mentions_t2.csv with 17 tweets.\n",
      "multiusers191213/patrick_edery_mentions_t2.csv with 21 tweets.\n",
      "multiusers191213/LSoubranne_mentions_t2.csv with 101 tweets.\n",
      "multiusers191213/DelarueJC_mentions_t2.csv with 46 tweets.\n",
      "multiusers191213/ThierryMARIANI_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/GuillaumeQ1T1_mentions_t2.csv with 148 tweets.\n",
      "multiusers191213/ogrenyx_mentions_t2.csv with 40 tweets.\n",
      "multiusers191213/arretsurimages_mentions_t2.csv with 8 tweets.\n",
      "multiusers191213/nico2312_mentions_t2.csv with 17 tweets.\n",
      "multiusers191213/enmarchefr_mentions_t2.csv with 9 tweets.\n",
      "multiusers191213/GPirouli_mentions_t2.csv with 92 tweets.\n",
      "multiusers191213/laurencedecock1_mentions_t2.csv with 29 tweets.\n",
      "multiusers191213/LysLady1_mentions_t2.csv with 41 tweets.\n",
      "multiusers191213/rakauto_mentions_t2.csv with 10 tweets.\n",
      "multiusers191213/lise__marie_mentions_t2.csv with 84 tweets.\n",
      "multiusers191213/lilmaouz_mentions_t2.csv with 11 tweets.\n",
      "multiusers191213/CFDT_mentions_t2.csv with 17 tweets.\n",
      "multiusers191213/zen31083632_mentions_t2.csv with 80 tweets.\n",
      "multiusers191213/ibalkany_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/JeanMessiha_mentions_t2.csv with 22 tweets.\n",
      "multiusers191213/GaveEmmanuelle_mentions_t2.csv with 30 tweets.\n",
      "multiusers191213/Courtoisix_mentions_t2.csv with 34 tweets.\n",
      "multiusers191213/fredileparigo_mentions_t2.csv with 30 tweets.\n",
      "multiusers191213/JstLangevin_mentions_t2.csv with 139 tweets.\n",
      "multiusers191213/AmezianeRose_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/Amandine9782_mentions_t2.csv with 55 tweets.\n",
      "multiusers191213/CgtTuifrance_mentions_t2.csv with 167 tweets.\n",
      "multiusers191213/melonland2017_mentions_t2.csv with 82 tweets.\n",
      "multiusers191213/aristogiton3_mentions_t2.csv with 69 tweets.\n",
      "multiusers191213/UKLabour_mentions_t2.csv with 60 tweets.\n",
      "multiusers191213/Francois_Ruffin_mentions_t2.csv with 2 tweets.\n",
      "multiusers191213/MLP_officiel_mentions_t2.csv with 7 tweets.\n",
      "multiusers191213/AlexndraBorchio_mentions_t2.csv with 10 tweets.\n",
      "multiusers191213/Blondie2714_mentions_t2.csv with 7 tweets.\n",
      "multiusers191213/le_Parisien_mentions_t2.csv with 31 tweets.\n",
      "multiusers191213/Miss_Nerdy88_mentions_t2.csv with 12 tweets.\n",
      "multiusers191213/LGBTQillyria_mentions_t2.csv with 56 tweets.\n",
      "multiusers191213/DesobeauxLaeti1_mentions_t2.csv with 76 tweets.\n",
      "multiusers191213/mimimatic_mentions_t2.csv with 44 tweets.\n",
      "multiusers191213/BFMParis_mentions_t2.csv with 15 tweets.\n",
      "multiusers191213/CfdtBerger_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/Brindmine1708_mentions_t2.csv with 70 tweets.\n",
      "multiusers191213/Mediapart_mentions_t2.csv with 28 tweets.\n",
      "multiusers191213/_MasonGreenwood_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/EmmWargon_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/passionsdelisah_mentions_t2.csv with 125 tweets.\n",
      "multiusers191213/riva_vitale_mentions_t2.csv with 3 tweets.\n",
      "multiusers191213/olivierbeaumont_mentions_t2.csv with 9 tweets.\n",
      "multiusers191213/Dr_moji_mentions_t2.csv with 14 tweets.\n",
      "multiusers191213/EmmacruelMacron_mentions_t2.csv with 102 tweets.\n",
      "multiusers191213/Marinalouloutte_mentions_t2.csv with 25 tweets.\n",
      "multiusers191213/Place_Beauvau_mentions_t2.csv with 18 tweets.\n",
      "multiusers191213/Zezette291_mentions_t2.csv with 54 tweets.\n",
      "multiusers191213/RobertJenrick_mentions_t2.csv with 8 tweets.\n",
      "multiusers191213/20hFrance2_mentions_t2.csv with 15 tweets.\n",
      "multiusers191213/Henri_B1011_mentions_t2.csv with 61 tweets.\n",
      "multiusers191213/RaquelGarridoFr_mentions_t2.csv with 22 tweets.\n",
      "multiusers191213/CouraudMaya_mentions_t2.csv with 211 tweets.\n",
      "multiusers191213/gillesjohnson_mentions_t2.csv with 57 tweets.\n",
      "multiusers191213/_Stalker_69__mentions_t2.csv with 145 tweets.\n",
      "multiusers191213/SudRailCentraux_mentions_t2.csv with 130 tweets.\n",
      "multiusers191213/Meridiana2008_mentions_t2.csv with 28 tweets.\n",
      "multiusers191213/CgtRatp_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/dymoney77_mentions_t2.csv with 51 tweets.\n",
      "multiusers191213/BDoowayst_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/bourg_d_mentions_t2.csv with 14 tweets.\n",
      "multiusers191213/MoniquePlaza3_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/SandraBks_mentions_t2.csv with 45 tweets.\n",
      "multiusers191213/aurelien_veron_mentions_t2.csv with 54 tweets.\n",
      "multiusers191213/DominiqueLUNEL_mentions_t2.csv with 59 tweets.\n",
      "multiusers191213/ManUtd_mentions_t2.csv with 15 tweets.\n",
      "multiusers191213/AARHOIODlAN_mentions_t2.csv with 51 tweets.\n",
      "multiusers191213/Forum_RTS_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/motolove66_mentions_t2.csv with 95 tweets.\n",
      "multiusers191213/eintracht_lz_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/kowakunai_kyo_mentions_t2.csv with 122 tweets.\n",
      "multiusers191213/fferaerts_mentions_t2.csv with 24 tweets.\n",
      "multiusers191213/CentristeModere_mentions_t2.csv with 54 tweets.\n",
      "multiusers191213/LEXPRESS_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/PierreMenes_mentions_t2.csv with 23 tweets.\n",
      "multiusers191213/IliasPanchard_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/Franck82170511_mentions_t2.csv with 49 tweets.\n",
      "multiusers191213/kkerima_mentions_t2.csv with 39 tweets.\n",
      "multiusers191213/LIGNER_SNCF_mentions_t2.csv with 67 tweets.\n",
      "multiusers191213/girlssmellweird_mentions_t2.csv with 85 tweets.\n",
      "multiusers191213/CNEWS_Sport_mentions_t2.csv with 3 tweets.\n",
      "multiusers191213/claudarnaud_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/EnModeMacaron_mentions_t2.csv with 2 tweets.\n",
      "multiusers191213/CSpontanees_mentions_t2.csv with 175 tweets.\n",
      "multiusers191213/anapunthanomai_mentions_t2.csv with 9 tweets.\n",
      "multiusers191213/Remifasol57_mentions_t2.csv with 23 tweets.\n",
      "multiusers191213/francefirst2_mentions_t2.csv with 100 tweets.\n",
      "multiusers191213/MargSchinas_mentions_t2.csv with 9 tweets.\n",
      "multiusers191213/LibDems_mentions_t2.csv with 41 tweets.\n",
      "multiusers191213/florencehenry56_mentions_t2.csv with 38 tweets.\n",
      "multiusers191213/RTSinfo_mentions_t2.csv with 29 tweets.\n",
      "multiusers191213/GaccioB_mentions_t2.csv with 41 tweets.\n",
      "multiusers191213/eric_guillaumin_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/FrancineCosimi1_mentions_t2.csv with 3 tweets.\n",
      "multiusers191213/KhaledFreak_mentions_t2.csv with 3 tweets.\n",
      "multiusers191213/NBelloubet_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/EuropaLeague_mentions_t2.csv with 22 tweets.\n",
      "multiusers191213/joswinson_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/frisson2com_mentions_t2.csv with 60 tweets.\n",
      "multiusers191213/Tsipora777_mentions_t2.csv with 146 tweets.\n",
      "multiusers191213/ESJLille_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/Alex451618_mentions_t2.csv with 68 tweets.\n",
      "multiusers191213/MalekDelegue_mentions_t2.csv with 111 tweets.\n",
      "multiusers191213/CousineauSamuel_mentions_t2.csv with 34 tweets.\n",
      "multiusers191213/leJDD_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/christine_kelly_mentions_t2.csv with 14 tweets.\n",
      "multiusers191213/beatricelecoz_mentions_t2.csv with 54 tweets.\n",
      "multiusers191213/Jade_Bekhti_mentions_t2.csv with 102 tweets.\n",
      "multiusers191213/19h30RTS_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/EPhilippePM_mentions_t2.csv with 2 tweets.\n",
      "multiusers191213/RaderSerge_mentions_t2.csv with 10 tweets.\n",
      "multiusers191213/watching_giants_mentions_t2.csv with 10 tweets.\n",
      "multiusers191213/llizygreen_mentions_t2.csv with 36 tweets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiusers191213/Hariulfi_Curtis_mentions_t2.csv with 38 tweets.\n",
      "multiusers191213/davidfayon_mentions_t2.csv with 7 tweets.\n",
      "multiusers191213/Hgibier_mentions_t2.csv with 51 tweets.\n",
      "multiusers191213/achabus_mentions_t2.csv with 158 tweets.\n",
      "multiusers191213/SansCompteFixe_mentions_t2.csv with 42 tweets.\n",
      "multiusers191213/YTCreateurs_mentions_t2.csv with 15 tweets.\n",
      "multiusers191213/monnier403_mentions_t2.csv with 72 tweets.\n",
      "multiusers191213/NPolony_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/C8TV_mentions_t2.csv with 72 tweets.\n",
      "multiusers191213/tombeaucombat_mentions_t2.csv with 113 tweets.\n",
      "multiusers191213/Mariett64686240_mentions_t2.csv with 49 tweets.\n",
      "multiusers191213/HarryCo91_mentions_t2.csv with 15 tweets.\n",
      "multiusers191213/Alvisyma_mentions_t2.csv with 127 tweets.\n",
      "multiusers191213/tegnererik_mentions_t2.csv with 29 tweets.\n",
      "multiusers191213/TradMethDealer_mentions_t2.csv with 39 tweets.\n",
      "multiusers191213/ConvDeLaDroite_mentions_t2.csv with 14 tweets.\n",
      "multiusers191213/JeanMar35370554_mentions_t2.csv with 47 tweets.\n",
      "multiusers191213/Fakir__mentions_t2.csv with 59 tweets.\n",
      "multiusers191213/Samuel_Lafont_mentions_t2.csv with 34 tweets.\n",
      "multiusers191213/KateRoc13_mentions_t2.csv with 45 tweets.\n",
      "multiusers191213/RNational_off_mentions_t2.csv with 19 tweets.\n",
      "multiusers191213/cocktail2Funk_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/Puck_Fair__mentions_t2.csv with 158 tweets.\n",
      "multiusers191213/TeamProgressist_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/Djebbari_JB_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/MarionMarechal_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/slappy_w_mentions_t2.csv with 15 tweets.\n",
      "multiusers191213/IamKVNPCM_mentions_t2.csv with 17 tweets.\n",
      "multiusers191213/frenchkaiser_mentions_t2.csv with 29 tweets.\n",
      "multiusers191213/MirkoToppano_mentions_t2.csv with 2 tweets.\n",
      "multiusers191213/GrandPalaisRmn_mentions_t2.csv with 8 tweets.\n",
      "multiusers191213/cresus1ier_mentions_t2.csv with 167 tweets.\n",
      "multiusers191213/cecile2menibus_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/HarryoFfm_mentions_t2.csv with 11 tweets.\n",
      "multiusers191213/tropical_boy_mentions_t2.csv with 139 tweets.\n",
      "multiusers191213/RTenfrancais_mentions_t2.csv with 43 tweets.\n",
      "multiusers191213/F_Desouche_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/ThFerrier_mentions_t2.csv with 51 tweets.\n",
      "multiusers191213/le_gorafi_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/EmRiposte_mentions_t2.csv with 41 tweets.\n",
      "multiusers191213/cfdtcheminots_mentions_t2.csv with 34 tweets.\n",
      "multiusers191213/ZemmourFaceInfo_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/StephaneAlbouy_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/Portes_Thomas_mentions_t2.csv with 19 tweets.\n",
      "multiusers191213/erichacquemand_mentions_t2.csv with 20 tweets.\n",
      "multiusers191213/LeTelegramme_mentions_t2.csv with 40 tweets.\n",
      "multiusers191213/VIGI_MI_mentions_t2.csv with 20 tweets.\n",
      "multiusers191213/zebodag_mentions_t2.csv with 90 tweets.\n",
      "multiusers191213/Valeurs_mentions_t2.csv with 15 tweets.\n",
      "multiusers191213/prontipronto_mentions_t2.csv with 12 tweets.\n",
      "multiusers191213/nicolasberrod_mentions_t2.csv with 26 tweets.\n",
      "multiusers191213/Damocles_Fr_mentions_t2.csv with 33 tweets.\n",
      "multiusers191213/MauriceMartin01_mentions_t2.csv with 131 tweets.\n",
      "multiusers191213/Montfreu_mentions_t2.csv with 28 tweets.\n",
      "multiusers191213/LibreRegis_mentions_t2.csv with 184 tweets.\n",
      "multiusers191213/Louisedescourt_mentions_t2.csv with 94 tweets.\n",
      "multiusers191213/Auvergnat_Deter_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/EmmanuelMacron_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/nenon0607_mentions_t2.csv with 181 tweets.\n",
      "multiusers191213/mimig1953_mentions_t2.csv with 27 tweets.\n",
      "multiusers191213/CCastaner_mentions_t2.csv with 13 tweets.\n",
      "multiusers191213/gregoryroose_mentions_t2.csv with 28 tweets.\n",
      "multiusers191213/cap_ou_pacap_mentions_t2.csv with 8 tweets.\n",
      "multiusers191213/HussonAlex_mentions_t2.csv with 19 tweets.\n",
      "multiusers191213/lesRepublicains_mentions_t2.csv with 39 tweets.\n",
      "multiusers191213/A22117890_mentions_t2.csv with 49 tweets.\n",
      "multiusers191213/natechonchon_mentions_t2.csv with 150 tweets.\n",
      "multiusers191213/J_Bardella_mentions_t2.csv with 19 tweets.\n",
      "multiusers191213/JLMelenchon_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/gouvernementFR_mentions_t2.csv with 9 tweets.\n",
      "multiusers191213/ChLECHEVALIER_mentions_t2.csv with 89 tweets.\n",
      "multiusers191213/Marieambre22_mentions_t2.csv with 29 tweets.\n",
      "multiusers191213/Vertumne1_mentions_t2.csv with 30 tweets.\n",
      "multiusers191213/solebesne_mentions_t2.csv with 161 tweets.\n",
      "multiusers191213/Napo1852_mentions_t2.csv with 25 tweets.\n",
      "multiusers191213/Pollueur__mentions_t2.csv with 8 tweets.\n",
      "multiusers191213/Studiodjip11_mentions_t2.csv with 12 tweets.\n",
      "multiusers191213/CNEWS_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/Sardoche_Lol_mentions_t2.csv with 47 tweets.\n",
      "multiusers191213/Vincent_Grrrr_mentions_t2.csv with 86 tweets.\n",
      "multiusers191213/larry_GVA_mentions_t2.csv with 7 tweets.\n",
      "multiusers191213/RachelChouchana_mentions_t2.csv with 184 tweets.\n",
      "multiusers191213/eridan626_mentions_t2.csv with 55 tweets.\n",
      "multiusers191213/jazzpote_mentions_t2.csv with 62 tweets.\n",
      "multiusers191213/AvecLisnard_mentions_t2.csv with 8 tweets.\n",
      "multiusers191213/nytimes_mentions_t2.csv with 45 tweets.\n",
      "multiusers191213/kreme2kassis_mentions_t2.csv with 105 tweets.\n",
      "multiusers191213/sylvita30_mentions_t2.csv with 157 tweets.\n",
      "multiusers191213/LauraPidcockMP_mentions_t2.csv with 33 tweets.\n",
      "multiusers191213/LucioleB5_mentions_t2.csv with 60 tweets.\n",
      "multiusers191213/AnasseKazib_mentions_t2.csv with 20 tweets.\n",
      "multiusers191213/BorisJohnson_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/FRpropagandan_mentions_t2.csv with 35 tweets.\n",
      "multiusers191213/_LePetitCaporal_mentions_t2.csv with 51 tweets.\n",
      "multiusers191213/alexandrecalvez_mentions_t2.csv with 3 tweets.\n",
      "multiusers191213/pascal5123_mentions_t2.csv with 47 tweets.\n",
      "multiusers191213/Valenti80776287_mentions_t2.csv with 35 tweets.\n",
      "multiusers191213/Annelie01190_mentions_t2.csv with 142 tweets.\n",
      "multiusers191213/TEYRAS1_mentions_t2.csv with 28 tweets.\n",
      "multiusers191213/caro_ligne_mentions_t2.csv with 12 tweets.\n",
      "multiusers191213/Jeuyl_mentions_t2.csv with 14 tweets.\n",
      "multiusers191213/MaryCherby_mentions_t2.csv with 172 tweets.\n",
      "multiusers191213/Remuverang1_mentions_t2.csv with 111 tweets.\n",
      "multiusers191213/catherinegaste_mentions_t2.csv with 30 tweets.\n",
      "multiusers191213/grognard_d_mentions_t2.csv with 66 tweets.\n",
      "multiusers191213/PenelopeB_mentions_t2.csv with 36 tweets.\n",
      "multiusers191213/KlimaschutzCH_mentions_t2.csv with 2 tweets.\n",
      "multiusers191213/Neurotherapeute_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/adia66_mentions_t2.csv with 107 tweets.\n",
      "multiusers191213/gotteswerk2411_mentions_t2.csv with 9 tweets.\n",
      "multiusers191213/Senat_mentions_t2.csv with 32 tweets.\n",
      "multiusers191213/francetvchine_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/Orhiecsou_mentions_t2.csv with 21 tweets.\n",
      "multiusers191213/Chabadalala_mentions_t2.csv with 91 tweets.\n",
      "multiusers191213/Triple_Donation_mentions_t2.csv with 71 tweets.\n",
      "multiusers191213/Triboque_mentions_t2.csv with 17 tweets.\n",
      "multiusers191213/ZeratoRSC2_mentions_t2.csv with 16 tweets.\n",
      "multiusers191213/Francoiszero2_mentions_t2.csv with 31 tweets.\n",
      "multiusers191213/pierrejovanovic_mentions_t2.csv with 79 tweets.\n",
      "multiusers191213/ArielNana26_mentions_t2.csv with 48 tweets.\n",
      "multiusers191213/RobertMenardFR_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/LegrandPersoml_mentions_t2.csv with 84 tweets.\n",
      "multiusers191213/BFMTV_mentions_t2.csv with 13 tweets.\n",
      "multiusers191213/DeSpartacus_mentions_t2.csv with 111 tweets.\n",
      "multiusers191213/GrueneCH_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/Eintracht_mentions_t2.csv with 21 tweets.\n",
      "multiusers191213/dupontaignan_mentions_t2.csv with 3 tweets.\n",
      "multiusers191213/DiciWashington_mentions_t2.csv with 12 tweets.\n",
      "multiusers191213/scania2235_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/PageBalkany_mentions_t2.csv with 13 tweets.\n",
      "multiusers191213/jeremycorbyn_mentions_t2.csv with 59 tweets.\n",
      "multiusers191213/LeParisien_75_mentions_t2.csv with 2 tweets.\n",
      "multiusers191213/Bassinrebel_mentions_t2.csv with 181 tweets.\n",
      "multiusers191213/anticor_org_mentions_t2.csv with 15 tweets.\n",
      "multiusers191213/Mnnbv13355811_mentions_t2.csv with 90 tweets.\n",
      "multiusers191213/ECiotti_mentions_t2.csv with 9 tweets.\n",
      "multiusers191213/realDonaldTrump_mentions_t2.csv with 1 tweets.\n",
      "multiusers191213/PNerval_mentions_t2.csv with 129 tweets.\n",
      "multiusers191213/apocalypto06_mentions_t2.csv with 45 tweets.\n",
      "multiusers191213/davidlisnard_mentions_t2.csv with 53 tweets.\n",
      "multiusers191213/sarroche_mentions_t2.csv with 4 tweets.\n",
      "multiusers191213/Conservatives_mentions_t2.csv with 57 tweets.\n",
      "multiusers191213/DMagiques_mentions_t2.csv with 17 tweets.\n",
      "multiusers191213/Beccah2Fois_mentions_t2.csv with 167 tweets.\n",
      "multiusers191213/MdAssilA_mentions_t2.csv with 12 tweets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiusers191213/GebekaFilms_mentions_t2.csv with 2 tweets.\n",
      "multiusers191213/PatrickRogerE_mentions_t2.csv with 9 tweets.\n",
      "multiusers191213/KelfaJ_mentions_t2.csv with 16 tweets.\n",
      "multiusers191213/MarianneleMag_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/Munsterma_mentions_t2.csv with 17 tweets.\n",
      "multiusers191213/Manifou_mentions_t2.csv with 123 tweets.\n",
      "multiusers191213/fleurdepee_mentions_t2.csv with 132 tweets.\n",
      "multiusers191213/MarchaisTh_mentions_t2.csv with 63 tweets.\n",
      "multiusers191213/carriesymonds_mentions_t2.csv with 10 tweets.\n",
      "multiusers191213/__Verlaine___mentions_t2.csv with 67 tweets.\n",
      "multiusers191213/mediaslibres_mentions_t2.csv with 3 tweets.\n",
      "multiusers191213/YlvaJohansson_mentions_t2.csv with 13 tweets.\n",
      "multiusers191213/HopitalC_mentions_t2.csv with 33 tweets.\n",
      "multiusers191213/Marie_dz_mentions_t2.csv with 145 tweets.\n",
      "multiusers191213/camusetfrederic_mentions_t2.csv with 39 tweets.\n",
      "multiusers191213/GWGoldnadel_mentions_t2.csv with 13 tweets.\n",
      "multiusers191213/DidierMaisto_mentions_t2.csv with 73 tweets.\n",
      "multiusers191213/AitiDouze_mentions_t2.csv with 34 tweets.\n",
      "multiusers191213/HeraklesSoter_mentions_t2.csv with 63 tweets.\n",
      "multiusers191213/TEAMPSGULTRA_mentions_t2.csv with 6 tweets.\n",
      "multiusers191213/jchribuisson_mentions_t2.csv with 11 tweets.\n",
      "multiusers191213/damienabad_mentions_t2.csv with 5 tweets.\n",
      "multiusers191213/AliveAndHereNow_mentions_t2.csv with 59 tweets.\n",
      "multiusers191213/flouppi_mentions_t2.csv with 81 tweets.\n",
      "multiusers191213/GBOU66_mentions_t2.csv with 86 tweets.\n",
      "multiusers191213/RMCinfo_mentions_t2.csv with 29 tweets.\n",
      "multiusers191213/elizabethaoust_mentions_t2.csv with 33 tweets.\n",
      "multiusers191213/elisahk92_mentions_t2.csv with 87 tweets.\n",
      "multiusers191213/MatthieuGariel_mentions_t2.csv with 129 tweets.\n",
      "multiusers191213/VincentVerier_mentions_t2.csv with 1 tweets.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "edge_df = pd.DataFrame()\n",
    "for filename in glob.glob(data_path + '*_mentions' +'_t' +str(thres)+ '.csv'):\n",
    "    new_edge_df = pd.read_csv(filename)\n",
    "    print('{} with {} tweets.'.format(filename,len(new_edge_df)))\n",
    "    edge_df = edge_df.append(new_edge_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user</th>\n",
       "      <th>mention</th>\n",
       "      <th>weight</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>date</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Elisabeth92390</td>\n",
       "      <td>ChloeLe39602964</td>\n",
       "      <td>2</td>\n",
       "      <td>['Dartmoor', 'Devon', 'Britain', 'trees', 'wat...</td>\n",
       "      <td>['2019-12-13 09:39:59', '2019-12-13 09:39:45']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Elisabeth92390</td>\n",
       "      <td>DanielPilotte</td>\n",
       "      <td>81</td>\n",
       "      <td>['EureEtLoir', 'SaintMichelChefChef', 'LoireAt...</td>\n",
       "      <td>['2019-12-13 14:45:59', '2019-12-13 14:45:38',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>Elisabeth92390</td>\n",
       "      <td>JeanMessiha</td>\n",
       "      <td>2</td>\n",
       "      <td>['Brexit', 'Trump']</td>\n",
       "      <td>['2019-12-13 11:45:39', '2019-12-13 09:08:48']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>Elisabeth92390</td>\n",
       "      <td>Marc_Urgaud</td>\n",
       "      <td>5</td>\n",
       "      <td>['LR', 'LR']</td>\n",
       "      <td>['2019-12-13 14:49:54', '2019-12-13 14:49:39',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>Elisabeth92390</td>\n",
       "      <td>gregoryroose</td>\n",
       "      <td>2</td>\n",
       "      <td>['Brexit', 'Brexit']</td>\n",
       "      <td>['2019-12-13 08:29:31', '2019-12-13 08:22:18']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>MatthieuGariel</td>\n",
       "      <td>GG_RMC</td>\n",
       "      <td>6</td>\n",
       "      <td>['SudRail', 'Villedieu', 'neuneu']</td>\n",
       "      <td>['2019-12-13 12:45:21', '2019-12-13 11:19:06',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>MatthieuGariel</td>\n",
       "      <td>MarleneSchiappa</td>\n",
       "      <td>5</td>\n",
       "      <td>['neuneu', 'Grève11Décembre', 'RATP']</td>\n",
       "      <td>['2019-12-12 13:04:08', '2019-12-12 12:44:35',...</td>\n",
       "      <td>['https://twitter.com/clientsratp/status/12048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>MatthieuGariel</td>\n",
       "      <td>didierandre94</td>\n",
       "      <td>2</td>\n",
       "      <td>['SudRail', 'Villedieu']</td>\n",
       "      <td>['2019-12-12 11:31:26', '2019-12-12 09:28:56']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>MatthieuGariel</td>\n",
       "      <td>prefpolice</td>\n",
       "      <td>3</td>\n",
       "      <td>['Asnières', 'Nanterre']</td>\n",
       "      <td>['2019-12-13 08:40:01', '2019-12-13 08:39:37',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>MatthieuGariel</td>\n",
       "      <td>tcabarrus</td>\n",
       "      <td>1</td>\n",
       "      <td>['PatrickCohen', 'LaurentBrun', 'CGT']</td>\n",
       "      <td>['2019-12-11 23:53:24']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0            user          mention  weight  \\\n",
       "16           16  Elisabeth92390  ChloeLe39602964       2   \n",
       "18           18  Elisabeth92390    DanielPilotte      81   \n",
       "31           31  Elisabeth92390      JeanMessiha       2   \n",
       "46           46  Elisabeth92390      Marc_Urgaud       5   \n",
       "81           81  Elisabeth92390     gregoryroose       2   \n",
       "..          ...             ...              ...     ...   \n",
       "35           35  MatthieuGariel           GG_RMC       6   \n",
       "58           58  MatthieuGariel  MarleneSchiappa       5   \n",
       "98           98  MatthieuGariel    didierandre94       2   \n",
       "119         119  MatthieuGariel       prefpolice       3   \n",
       "124         124  MatthieuGariel        tcabarrus       1   \n",
       "\n",
       "                                              hashtags  \\\n",
       "16   ['Dartmoor', 'Devon', 'Britain', 'trees', 'wat...   \n",
       "18   ['EureEtLoir', 'SaintMichelChefChef', 'LoireAt...   \n",
       "31                                 ['Brexit', 'Trump']   \n",
       "46                                        ['LR', 'LR']   \n",
       "81                                ['Brexit', 'Brexit']   \n",
       "..                                                 ...   \n",
       "35                  ['SudRail', 'Villedieu', 'neuneu']   \n",
       "58               ['neuneu', 'Grève11Décembre', 'RATP']   \n",
       "98                            ['SudRail', 'Villedieu']   \n",
       "119                           ['Asnières', 'Nanterre']   \n",
       "124             ['PatrickCohen', 'LaurentBrun', 'CGT']   \n",
       "\n",
       "                                                  date  \\\n",
       "16      ['2019-12-13 09:39:59', '2019-12-13 09:39:45']   \n",
       "18   ['2019-12-13 14:45:59', '2019-12-13 14:45:38',...   \n",
       "31      ['2019-12-13 11:45:39', '2019-12-13 09:08:48']   \n",
       "46   ['2019-12-13 14:49:54', '2019-12-13 14:49:39',...   \n",
       "81      ['2019-12-13 08:29:31', '2019-12-13 08:22:18']   \n",
       "..                                                 ...   \n",
       "35   ['2019-12-13 12:45:21', '2019-12-13 11:19:06',...   \n",
       "58   ['2019-12-12 13:04:08', '2019-12-12 12:44:35',...   \n",
       "98      ['2019-12-12 11:31:26', '2019-12-12 09:28:56']   \n",
       "119  ['2019-12-13 08:40:01', '2019-12-13 08:39:37',...   \n",
       "124                            ['2019-12-11 23:53:24']   \n",
       "\n",
       "                                                  urls  \n",
       "16                                                  []  \n",
       "18                                                  []  \n",
       "31                                                  []  \n",
       "46                                                  []  \n",
       "81                                                  []  \n",
       "..                                                 ...  \n",
       "35                                                  []  \n",
       "58   ['https://twitter.com/clientsratp/status/12048...  \n",
       "98                                                  []  \n",
       "119                                                 []  \n",
       "124                                                 []  \n",
       "\n",
       "[1460 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display edges with number of hashtags >1\n",
    "edge_df[edge_df['hashtags'].apply(lambda x : len(x.split()))>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def graph_from_edgeslist(edge_df,degree_min):\n",
    "    print('Creating the graph fro th edge list')\n",
    "    G = nx.from_pandas_edgelist(edge_df,source='user',target='mention', edge_attr=['weight','hashtags','date','urls'])\n",
    "    print('Nb of nodes:',G.number_of_nodes())\n",
    "    # Drop\n",
    "    remove = [node for node,degree in dict(G.degree()).items() if degree < degree_min]\n",
    "    G.remove_nodes_from(remove)\n",
    "    print('Nb of nodes after removing less connected nodes:',G.number_of_nodes())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the graph fro th edge list\n",
      "Nb of nodes: 7482\n",
      "Nb of nodes after removing less connected nodes: 1183\n",
      "removed 2 isolated nodes.\n"
     ]
    }
   ],
   "source": [
    "DEGREE_MIN = 3 # Minimal number of connections in the graph\n",
    "\n",
    "G = graph_from_edgeslist(edge_df,DEGREE_MIN)\n",
    "isolates = list(nx.isolates(G))\n",
    "G.remove_nodes_from(isolates)\n",
    "print('removed {} isolated nodes.'.format(len(isolates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of partitions: 15\n"
     ]
    }
   ],
   "source": [
    "#first compute the best partition\n",
    "partition = community.best_partition(G)\n",
    "nx.set_node_attributes(G,partition,name='community')\n",
    "nb_partitions = max(partition.values())+1\n",
    "print('Nb of partitions:',nb_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphname = 'multiusersgraph'\n",
    "#graphname = 'GBRgraph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote multiusers191213/multiusersgraph_t2_graph.gexf\n"
     ]
    }
   ],
   "source": [
    "# Save the graph\n",
    "graphfilename = data_path + graphname + '_t' + str(thres) +'_graph.gexf'\n",
    "nx.write_gexf(G,graphfilename)\n",
    "print('Wrote',graphfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags, dates and urls\n",
    "Hashtags, dates and urls are on the edges of the network.\n",
    "We can get the most common hashtags within a community and also betwenn communities using the edges that connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast # convert string to list\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the hashtags for each community and inter-communities\n",
    "tags_dic = {}\n",
    "dates_dic = {}\n",
    "url_dic = {}\n",
    "for node1,node2,data in G.edges(data=True):\n",
    "    if node1 == node2:\n",
    "        print('Self edge',node1)\n",
    "    n1_com = G.nodes[node1]['community']\n",
    "    n2_com = G.nodes[node2]['community']\n",
    "    new_key = str(n1_com) + '-' + str(n2_com) # intra / inter community code\n",
    "    # Convert string to list\n",
    "    x = ast.literal_eval(data['hashtags'])\n",
    "    d = ast.literal_eval(data['date'])\n",
    "    u = ast.literal_eval(data['urls'])\n",
    "    keywords = [n.strip() for n in x]\n",
    "    date_list = [n.strip() for n in d]\n",
    "    urls = [n.strip() for n in u]\n",
    "    if new_key not in tags_dic:\n",
    "        tags_dic[new_key] = keywords\n",
    "        dates_dic[new_key] = date_list\n",
    "        url_dic[new_key] = urls\n",
    "    else:\n",
    "        tags_dic[new_key] += keywords \n",
    "        dates_dic[new_key] += date_list\n",
    "        url_dic[new_key] += urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the most common hashtags in communities and inter communities\n",
    "#for key in tags_dic:\n",
    "#    most_common = Counter(tags_dic[key]).most_common(5)\n",
    "#    print(key)\n",
    "#    print(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meantime(date_list):\n",
    "    d_list = [ datetime.datetime.strptime(dt,'%Y-%m-%d %H:%M:%S') for dt in date_list]\n",
    "    second_list = [x.timestamp() for x in d_list]\n",
    "    meand = np.mean(second_list)\n",
    "    stdd = np.std(second_list)\n",
    "    return datetime.datetime.fromtimestamp(meand),datetime.timedelta(seconds=stdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a table with time and hashtags for each community\n",
    "comm_list = []\n",
    "for key in np.arange(nb_partitions):\n",
    "    keykey = str(key)+ '-' +str(key)\n",
    "    most_common = Counter(tags_dic[keykey]).most_common(5)\n",
    "    meandate,stddate = compute_meantime(dates_dic[keykey])\n",
    "    #print('Community',key)\n",
    "    #print(most_common)\n",
    "    #print('Average date: {} and std deviation: {} days'.format(meandate.date(),stddate.days))\n",
    "    comm_dic = {'Community':key, 'Average date':meandate.date(), 'Deviation (days)':stddate.days}\n",
    "    for htag_nb in range(5): # filling the table with the hashtags\n",
    "        if htag_nb < len(most_common):\n",
    "            comm_dic['hashtag'+str(htag_nb)] = most_common[htag_nb][0]\n",
    "        else:\n",
    "            comm_dic['hashtag'+str(htag_nb)] = ''\n",
    "    comm_list.append(comm_dic)\n",
    "community_table = pd.DataFrame(comm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Community</th>\n",
       "      <th>Average date</th>\n",
       "      <th>Deviation (days)</th>\n",
       "      <th>hashtag0</th>\n",
       "      <th>hashtag1</th>\n",
       "      <th>hashtag2</th>\n",
       "      <th>hashtag3</th>\n",
       "      <th>hashtag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>0</td>\n",
       "      <td>macguignol</td>\n",
       "      <td>BoycottNike</td>\n",
       "      <td>Zemmour</td>\n",
       "      <td>Apathie</td>\n",
       "      <td>EureEtLoir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>LR</td>\n",
       "      <td>retraites</td>\n",
       "      <td>Cannes</td>\n",
       "      <td>Zemmour</td>\n",
       "      <td>Retraites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>GGRMC</td>\n",
       "      <td>SudRadio</td>\n",
       "      <td>Grèves</td>\n",
       "      <td>grevedu13decembre</td>\n",
       "      <td>GrandMatinSudRadio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Retraites</td>\n",
       "      <td>Macron</td>\n",
       "      <td>greve12decembre</td>\n",
       "      <td>retraites</td>\n",
       "      <td>Edouardphilippe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Bedford</td>\n",
       "      <td>GeneralElection2019</td>\n",
       "      <td>19h30RTS</td>\n",
       "      <td>Brexit</td>\n",
       "      <td>UKElection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>ParisCentre</td>\n",
       "      <td>RATP</td>\n",
       "      <td>Ghali</td>\n",
       "      <td>guepe</td>\n",
       "      <td>futuraplanete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>0</td>\n",
       "      <td>Levallois</td>\n",
       "      <td>Israel</td>\n",
       "      <td>teambalkany</td>\n",
       "      <td>Balkany</td>\n",
       "      <td>Nike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>22mai</td>\n",
       "      <td>Delevoye</td>\n",
       "      <td>retraites</td>\n",
       "      <td>oubli</td>\n",
       "      <td>ReformeRetraites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>0</td>\n",
       "      <td>BalanceTonPost</td>\n",
       "      <td>OlympiaAwards</td>\n",
       "      <td>TPMP</td>\n",
       "      <td>BTP</td>\n",
       "      <td>CapSizun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>France</td>\n",
       "      <td>Macron</td>\n",
       "      <td>migrant</td>\n",
       "      <td>Vedène</td>\n",
       "      <td>amende</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>WatchingGiants</td>\n",
       "      <td>murdelahonte</td>\n",
       "      <td>opVA</td>\n",
       "      <td>RWC2019</td>\n",
       "      <td>Brexit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>TheGameAwards</td>\n",
       "      <td>TF2</td>\n",
       "      <td>GameAwards</td>\n",
       "      <td>UKElection</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>TeamLigneR</td>\n",
       "      <td>RERD</td>\n",
       "      <td>SOSusagers</td>\n",
       "      <td>GrèveBlocage</td>\n",
       "      <td>LigneR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>0</td>\n",
       "      <td>UEL</td>\n",
       "      <td>MUFC</td>\n",
       "      <td>SGEuropa</td>\n",
       "      <td>SGEVSC</td>\n",
       "      <td>WelcomeGattuso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Community Average date  Deviation (days)        hashtag0  \\\n",
       "0           0   2019-12-13                 0      macguignol   \n",
       "1           1   2019-12-12                 0              LR   \n",
       "2           2   2019-12-12                 0           GGRMC   \n",
       "3           3   2019-12-12                 0       Retraites   \n",
       "4           4   2019-12-12                 0         Bedford   \n",
       "5           5   2019-12-12                 0     ParisCentre   \n",
       "6           6   2019-12-13                 0       Levallois   \n",
       "7           7   2019-12-12                 0           22mai   \n",
       "8           8   2019-12-13                 0  BalanceTonPost   \n",
       "9           9   2019-12-12                 0          France   \n",
       "10         10   2019-12-12                 0  WatchingGiants   \n",
       "11         11   2019-12-12                 0   TheGameAwards   \n",
       "12         12   2019-12-12                 0      TeamLigneR   \n",
       "13         13   2019-12-12                 0             UEL   \n",
       "\n",
       "               hashtag1         hashtag2           hashtag3  \\\n",
       "0           BoycottNike          Zemmour            Apathie   \n",
       "1             retraites           Cannes            Zemmour   \n",
       "2              SudRadio           Grèves  grevedu13decembre   \n",
       "3                Macron  greve12decembre          retraites   \n",
       "4   GeneralElection2019         19h30RTS             Brexit   \n",
       "5                  RATP            Ghali              guepe   \n",
       "6                Israel      teambalkany            Balkany   \n",
       "7              Delevoye        retraites              oubli   \n",
       "8         OlympiaAwards             TPMP                BTP   \n",
       "9                Macron          migrant             Vedène   \n",
       "10         murdelahonte             opVA            RWC2019   \n",
       "11                  TF2       GameAwards         UKElection   \n",
       "12                 RERD       SOSusagers       GrèveBlocage   \n",
       "13                 MUFC         SGEuropa             SGEVSC   \n",
       "\n",
       "              hashtag4  \n",
       "0           EureEtLoir  \n",
       "1            Retraites  \n",
       "2   GrandMatinSudRadio  \n",
       "3      Edouardphilippe  \n",
       "4           UKElection  \n",
       "5        futuraplanete  \n",
       "6                 Nike  \n",
       "7     ReformeRetraites  \n",
       "8             CapSizun  \n",
       "9               amende  \n",
       "10              Brexit  \n",
       "11                      \n",
       "12              LigneR  \n",
       "13      WelcomeGattuso  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the url of each cluster and inter-clusters\n",
    "urltocomm = []\n",
    "for key in url_dic:\n",
    "    for url in url_dic[key]:\n",
    "        urltocomm.append([url,key,1])\n",
    "url_table = pd.DataFrame(urltocomm, columns=['url','Community','Occurence'])\n",
    "url_table = url_table.groupby(['url','Community']).agg(Occurence=('Occurence',sum))\n",
    "url_table = url_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all bit.ly url by the correct one\n",
    "import requests\n",
    "\n",
    "session = requests.Session()  # so connections are recycled\n",
    "\n",
    "for index, row in url_table.iterrows():\n",
    "    url = row['url']\n",
    "    if 'bit.ly' in url:\n",
    "        resp = session.head(url, allow_redirects=True)\n",
    "        url_table.loc[index,'url'] = resp.url\n",
    "        #print(resp.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the references to twitter web site\n",
    "twitterrowindices = url_table[url_table['url'].str.contains('twitter.com')].index\n",
    "filtered_url_table = url_table.drop(twitterrowindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the intra community links\n",
    "dropindices = []\n",
    "for index, row in filtered_url_table.iterrows():\n",
    "    if row['Community'][0] != row['Community'][-1]:\n",
    "        dropindices.append(index)\n",
    "    else: # modify the entry\n",
    "        filtered_url_table.loc[index,'Community'] = row['Community'][0]\n",
    "filtered_url_table = filtered_url_table.drop(dropindices)\n",
    "filtered_url_table.reset_index(inplace=True)\n",
    "filtered_url_table.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort users by community and save in a excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort users by community and store their node degree (importance)\n",
    "community_nodes = {}\n",
    "for node,data in G.nodes(data=True):\n",
    "    community_nb = data['community']\n",
    "    if  community_nb not in community_nodes:\n",
    "        community_nodes[community_nb] = [(node, G.degree(node))]\n",
    "    else:\n",
    "        community_nodes[community_nb].append((node, G.degree(node)))\n",
    "\n",
    "\n",
    "# Display the exmaple of community c_idx\n",
    "#c_idx = 0\n",
    "#ddf = pd.DataFrame(community_nodes[c_idx],columns=['User','Degree'])\n",
    "#print('list of most connected users in community',c_idx)\n",
    "#ddf.sort_values(by='Degree',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to an excel file\n",
    "with pd.ExcelWriter(data_path + 'graph_infos.xlsx') as writer:\n",
    "    for community_nb in community_nodes:\n",
    "        ddf = pd.DataFrame(community_nodes[community_nb],columns=['User','Degree'])\n",
    "        ddf = ddf.sort_values(by='Degree',ascending=False)#.head(20)\n",
    "        ddf.to_excel(writer, sheet_name='Community_' + str(community_nb),index=False)\n",
    "    community_table.to_excel(writer, sheet_name='Hashtags',index=False)\n",
    "    users_df.to_excel(writer, sheet_name='Initial_users_details',index=False)\n",
    "    filtered_url_table.to_excel(writer, sheet_name='List_of_urls',index=False)\n",
    "    # Set the column width\n",
    "    column_width = 25\n",
    "    for sheet in writer.sheets: \n",
    "        worksheet = writer.sheets[sheet]\n",
    "        for col in ['A','B','C','D','E','F','G','H']:\n",
    "            worksheet.column_dimensions[col].width = column_width\n",
    "    writer.sheets['List_of_urls'].column_dimensions['A'].width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
