{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Twython class\n",
    "from twython import Twython\n",
    "import json\n",
    "\n",
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)\n",
    "\n",
    "# Instantiate an object\n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date string = 191216\n",
      "Path created: immigration_reduced191216/\n"
     ]
    }
   ],
   "source": [
    "#username = 'templivs'\n",
    "#username_list  = ['GilbertCollard','dav_dec','Carbongate','bcassoret',\n",
    "#                  'Electroversenet','thinkfree55', 'KlassLib','sauvonsleclimat']\n",
    "\n",
    "username_list = ['francisrichard','MazdaArtaxerxes','templivs','prontipronto',\n",
    "                'Chabadalala','cocktail2Funk','HopitalC',\n",
    "                'riva_vitale','Remifasol57','AitiDouze', 'QAnonAustria1', 'gotteswerk2411']\n",
    "\n",
    "swiss_accounts = ['KlimaschutzCH', 'GrueneCH', 'proclimCH', 'EperonP', 'MathiasTemujin',\n",
    "                  'klimastreik', 'AlimEquitables', 'ProNaturaSuisse', 'vertliberaux', 'Munsterma',\n",
    "                  'bourg_d', 'LesVertsSuisses', 'ClimatSuisse', 'gpsuisse', 'IliasPanchard', 'ATE_Suisse']\n",
    "\n",
    "immigration = ['Kalvingrad1291','democratesuisse','VigilanceIslam','lioneljonson01','ChWilhou',\n",
    "               'HunterSThomson2','A_Addams_','ObservateursCH','JuanCandida','novopress']\n",
    "\n",
    "username_list = immigration\n",
    "# create the path to save the experiment indexed with a date\n",
    "today = date.today()\n",
    "date_string = today.strftime(\"%y%m%d\")\n",
    "print(\"date string =\", date_string)\n",
    "\n",
    "#date_string = '191128'\n",
    "\n",
    "data_path = 'immigration_reduced' + date_string+ '/'\n",
    "#get_tweets = python_tweets.get_user_timeline(screen_name = username,  \n",
    "#                                           count = 200, include_rts = True)\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n",
    "    print('Path created:',data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pysad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 2 # minimal number of mentions to keep\n",
    "max_day_old = 2 # number max of days in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_list(python_tweets, data_path, username, thres=3, max_day_old=None):\n",
    "    users_dic = {'username':[], 'Nb_diff_mentions': []}\n",
    "    print('Collecting the tweets for the last {} days.'.format(max_day_old))\n",
    "    new_users_list = []\n",
    "    for user in username_list:\n",
    "        mentions = create_user_edgelist(python_tweets, data_path, user, thres=thres, max_day_old=max_day_old)\n",
    "        if not mentions.empty:\n",
    "            users_mentioned = mentions['mention'][mentions['weight']>thres]\n",
    "            #users_mentioned = users_mentioned.unique() # not sure this is useful\n",
    "            new_users_list += users_mentioned.tolist()\n",
    "        users_dic['username'].append(user)\n",
    "        users_dic['Nb_diff_mentions'].append(len(mentions))\n",
    "    users_df = pd.DataFrame(users_dic)\n",
    "    return new_users_list,users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_user_edgelist(python_tweets, data_path, username, thres, max_day_old):\n",
    "\t# Process the user username and its mentioned users\n",
    "\t# save in a file the edgelist for the user and each mentioned user\n",
    "\n",
    "\t# initial user\n",
    "\tprint('Processing',username)\n",
    "\t#try:\n",
    "\tmention_grouped,mgl = pysad.collect_user_mention(username,python_tweets,data_path, max_day_old=max_day_old)\n",
    "\t#except:\n",
    "\t#    print('exception catched on user {} !!!!!!!!!!!!'.format(username))\n",
    "\t#    return\n",
    "\tif mention_grouped.empty:\n",
    "\t\tprint('Empty tweet list. Processing stopped for user ',username)\n",
    "\t\treturn mention_grouped\n",
    "\tmentionfilename = data_path + username + '_mentions' +'_t' +str(thres)+'.csv'\n",
    "\tprint('Writing {} tweets in {}.'.format(len(mention_grouped),mentionfilename))\n",
    "\tmention_grouped.to_csv(mentionfilename)\n",
    "\t#nb_mentions = len(mention_grouped)\n",
    "\t#print('User {} done. Nb different mentions: {}'.format(username,nb_mentions))\n",
    "\treturn mention_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting the tweets for the last 2 days.\n",
      "\n",
      "******* Processing users at 0-hop distance *******\n",
      "Collecting the tweets for the last 2 days.\n",
      "Processing Kalvingrad1291\n",
      "Empty tweet list. Processing stopped for user  Kalvingrad1291\n",
      "Processing democratesuisse\n",
      "Empty tweet list. Processing stopped for user  democratesuisse\n",
      "Processing VigilanceIslam\n",
      "Empty tweet list. Processing stopped for user  VigilanceIslam\n",
      "Processing lioneljonson01\n",
      "Writing 9 tweets in immigration_reduced191216/lioneljonson01_mentions_t2.csv.\n",
      "Processing ChWilhou\n",
      "Writing 8 tweets in immigration_reduced191216/ChWilhou_mentions_t2.csv.\n",
      "Processing HunterSThomson2\n",
      "Writing 75 tweets in immigration_reduced191216/HunterSThomson2_mentions_t2.csv.\n",
      "Processing A_Addams_\n",
      "Writing 9 tweets in immigration_reduced191216/A_Addams__mentions_t2.csv.\n",
      "Processing ObservateursCH\n",
      "Empty tweet list. Processing stopped for user  ObservateursCH\n",
      "Processing JuanCandida\n",
      "Empty tweet list. Processing stopped for user  JuanCandida\n",
      "Processing novopress\n",
      "Empty tweet list. Processing stopped for user  novopress\n",
      "\n",
      "******* Processing users at 1-hop distance *******\n",
      "Collecting the tweets for the last 2 days.\n",
      "Processing dr_l_alexandre\n",
      "Writing 6 tweets in immigration_reduced191216/dr_l_alexandre_mentions_t2.csv.\n",
      "Processing PaulFortune1975\n",
      "Writing 21 tweets in immigration_reduced191216/PaulFortune1975_mentions_t2.csv.\n",
      "Processing KimJongUnique\n",
      "Writing 27 tweets in immigration_reduced191216/KimJongUnique_mentions_t2.csv.\n",
      "Processing holy_Phoenixx\n",
      "Writing 16 tweets in immigration_reduced191216/holy_Phoenixx_mentions_t2.csv.\n",
      "Processing tprincedelamour\n",
      "Writing 3 tweets in immigration_reduced191216/tprincedelamour_mentions_t2.csv.\n",
      "Processing JulienPain\n",
      "Writing 6 tweets in immigration_reduced191216/JulienPain_mentions_t2.csv.\n",
      "Processing raumnitz\n",
      "Writing 16 tweets in immigration_reduced191216/raumnitz_mentions_t2.csv.\n",
      "Processing FSeclit\n",
      "Writing 1 tweets in immigration_reduced191216/FSeclit_mentions_t2.csv.\n",
      "Processing TimeMachine1980\n",
      "Writing 75 tweets in immigration_reduced191216/TimeMachine1980_mentions_t2.csv.\n",
      "Processing CretinusAlp\n",
      "Writing 18 tweets in immigration_reduced191216/CretinusAlp_mentions_t2.csv.\n",
      "Processing S_o_l_s_t_i_c_e\n",
      "Writing 38 tweets in immigration_reduced191216/S_o_l_s_t_i_c_e_mentions_t2.csv.\n",
      "Processing gregtabibian\n",
      "Writing 7 tweets in immigration_reduced191216/gregtabibian_mentions_t2.csv.\n",
      "Processing Alastor750\n",
      "Writing 45 tweets in immigration_reduced191216/Alastor750_mentions_t2.csv.\n",
      "Processing SainteOraora\n",
      "Writing 81 tweets in immigration_reduced191216/SainteOraora_mentions_t2.csv.\n",
      "Processing T_Nenninger\n",
      "Writing 66 tweets in immigration_reduced191216/T_Nenninger_mentions_t2.csv.\n",
      "Processing PsykoSauce\n",
      "Writing 2 tweets in immigration_reduced191216/PsykoSauce_mentions_t2.csv.\n"
     ]
    }
   ],
   "source": [
    "users_dic = {'username':[], 'Nb_mentions': [], 'mentions_of_mentions': []}\n",
    "print('Collecting the tweets for the last {} days.'.format(max_day_old))\n",
    "exploration_depth = 2\n",
    "total_username_list = username_list\n",
    "for depth in range(exploration_depth):\n",
    "    print('')\n",
    "    print('******* Processing users at {}-hop distance *******'.format(depth))\n",
    "    new_users_list,users_df = process_user_list(python_tweets, data_path, username_list, thres=thres, max_day_old=max_day_old)\n",
    "    #New users to collect:\n",
    "    username_list = list(set(new_users_list).difference(set(total_username_list))) # remove the one already collected\n",
    "    total_username_list += username_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users collected:\n",
      "102 102\n"
     ]
    }
   ],
   "source": [
    "print('Total number of users collected:')\n",
    "print(len(total_username_list),len(set(total_username_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved data into an edge table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immigration_reduced191216/A_Addams__mentions_t2.csv with 9 tweets.\n",
      "immigration_reduced191216/S_o_l_s_t_i_c_e_mentions_t2.csv with 38 tweets.\n",
      "immigration_reduced191216/PsykoSauce_mentions_t2.csv with 2 tweets.\n",
      "immigration_reduced191216/SainteOraora_mentions_t2.csv with 81 tweets.\n",
      "immigration_reduced191216/KimJongUnique_mentions_t2.csv with 27 tweets.\n",
      "immigration_reduced191216/TimeMachine1980_mentions_t2.csv with 75 tweets.\n",
      "immigration_reduced191216/PaulFortune1975_mentions_t2.csv with 21 tweets.\n",
      "immigration_reduced191216/T_Nenninger_mentions_t2.csv with 66 tweets.\n",
      "immigration_reduced191216/holy_Phoenixx_mentions_t2.csv with 16 tweets.\n",
      "immigration_reduced191216/gregtabibian_mentions_t2.csv with 7 tweets.\n",
      "immigration_reduced191216/raumnitz_mentions_t2.csv with 16 tweets.\n",
      "immigration_reduced191216/dr_l_alexandre_mentions_t2.csv with 6 tweets.\n",
      "immigration_reduced191216/JulienPain_mentions_t2.csv with 6 tweets.\n",
      "immigration_reduced191216/lioneljonson01_mentions_t2.csv with 9 tweets.\n",
      "immigration_reduced191216/tprincedelamour_mentions_t2.csv with 3 tweets.\n",
      "immigration_reduced191216/CretinusAlp_mentions_t2.csv with 18 tweets.\n",
      "immigration_reduced191216/HunterSThomson2_mentions_t2.csv with 75 tweets.\n",
      "immigration_reduced191216/ChWilhou_mentions_t2.csv with 8 tweets.\n",
      "immigration_reduced191216/FSeclit_mentions_t2.csv with 1 tweets.\n",
      "immigration_reduced191216/Alastor750_mentions_t2.csv with 45 tweets.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "edge_df = pd.DataFrame()\n",
    "for filename in glob.glob(data_path + '*_mentions' +'_t' +str(thres)+ '.csv'):\n",
    "    new_edge_df = pd.read_csv(filename)\n",
    "    print('{} with {} tweets.'.format(filename,len(new_edge_df)))\n",
    "    edge_df = edge_df.append(new_edge_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user</th>\n",
       "      <th>mention</th>\n",
       "      <th>weight</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>date</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>AllisonLeeSolin</td>\n",
       "      <td>1</td>\n",
       "      <td>['cdnpoli', 'debatimmigration']</td>\n",
       "      <td>['2019-12-14 23:21:01']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>Babs_cat33</td>\n",
       "      <td>1</td>\n",
       "      <td>['blacktwitter', 'racism']</td>\n",
       "      <td>['2019-12-15 23:21:01']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>InstitutILIADE</td>\n",
       "      <td>1</td>\n",
       "      <td>['blacktwitter', 'racism']</td>\n",
       "      <td>['2019-12-15 23:21:01']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>RenaudCamus</td>\n",
       "      <td>1</td>\n",
       "      <td>['blacktwitter', 'racism']</td>\n",
       "      <td>['2019-12-15 23:21:01']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>RevueInAnalysis</td>\n",
       "      <td>1</td>\n",
       "      <td>['cdnpoli', 'debatimmigration']</td>\n",
       "      <td>['2019-12-14 23:21:01']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>TheeAndroidRulz</td>\n",
       "      <td>1</td>\n",
       "      <td>['nrc', 'americadivided']</td>\n",
       "      <td>['2019-12-15 11:21:03']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>aires_aston</td>\n",
       "      <td>1</td>\n",
       "      <td>['nrc', 'americadivided']</td>\n",
       "      <td>['2019-12-15 11:21:03']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>rick_hoyer</td>\n",
       "      <td>1</td>\n",
       "      <td>['cdnpoli', 'debatimmigration']</td>\n",
       "      <td>['2019-12-14 23:21:01']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A_Addams_</td>\n",
       "      <td>truckster1</td>\n",
       "      <td>1</td>\n",
       "      <td>['nrc', 'americadivided']</td>\n",
       "      <td>['2019-12-15 11:21:03']</td>\n",
       "      <td>['https://paper.li/A_Addams_/1421662602?editio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>S_o_l_s_t_i_c_e</td>\n",
       "      <td>Chris_A10_USA</td>\n",
       "      <td>1</td>\n",
       "      <td>['Ireland', 'migrant', 'insanity']</td>\n",
       "      <td>['2019-12-15 21:25:47']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>S_o_l_s_t_i_c_e</td>\n",
       "      <td>F_Desouche</td>\n",
       "      <td>1</td>\n",
       "      <td>['Belgique', 'Berchem']</td>\n",
       "      <td>['2019-12-14 23:16:29']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>S_o_l_s_t_i_c_e</td>\n",
       "      <td>ValentiniBack</td>\n",
       "      <td>6</td>\n",
       "      <td>['GregoireJunod', 'GregoireJunod']</td>\n",
       "      <td>['2019-12-16 07:25:10', '2019-12-16 07:24:36',...</td>\n",
       "      <td>['https://lesobservateurs.ch/2019/12/16/italie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>S_o_l_s_t_i_c_e</td>\n",
       "      <td>jgremise</td>\n",
       "      <td>1</td>\n",
       "      <td>['Occitanie', 'Toulouse']</td>\n",
       "      <td>['2019-12-14 23:14:09']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>SainteOraora</td>\n",
       "      <td>rosylilacpetal</td>\n",
       "      <td>1</td>\n",
       "      <td>['jjbafanart', 'sbr', 'steelballrun', 'gyjo', ...</td>\n",
       "      <td>['2019-12-15 19:21:33']</td>\n",
       "      <td>['https://twitter.com/notcorrectjjba/status/12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>KimJongUnique</td>\n",
       "      <td>F_Desouche</td>\n",
       "      <td>1</td>\n",
       "      <td>['Morbihan', 'préfet']</td>\n",
       "      <td>['2019-12-15 22:09:36']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>TimeMachine1980</td>\n",
       "      <td>PresidenceNiger</td>\n",
       "      <td>1</td>\n",
       "      <td>['Niamey', 'G5Sahel']</td>\n",
       "      <td>['2019-12-15 15:52:42']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>TimeMachine1980</td>\n",
       "      <td>ROM1LeFrancc</td>\n",
       "      <td>12</td>\n",
       "      <td>['MissFrance2020', 'MissFrance2020']</td>\n",
       "      <td>['2019-12-15 15:36:18', '2019-12-15 15:35:51',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>T_Nenninger</td>\n",
       "      <td>AiphanMarcel</td>\n",
       "      <td>1</td>\n",
       "      <td>['SibethNdiaye', 'retraites']</td>\n",
       "      <td>['2019-12-15 22:40:44']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>T_Nenninger</td>\n",
       "      <td>Drmartyufml</td>\n",
       "      <td>1</td>\n",
       "      <td>['ReformeRetraites', 'RetraiteParPoints']</td>\n",
       "      <td>['2019-12-14 14:02:20']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tprincedelamour</td>\n",
       "      <td>CNEWS</td>\n",
       "      <td>1</td>\n",
       "      <td>['Dignes', 'Noel']</td>\n",
       "      <td>['2019-12-14 18:07:27']</td>\n",
       "      <td>['https://www.cnews.fr/videos/france/2019-12-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>HunterSThomson2</td>\n",
       "      <td>FSeclit</td>\n",
       "      <td>3</td>\n",
       "      <td>['interview', 'misogyne', 'moustache', 'MeToo']</td>\n",
       "      <td>['2019-12-16 09:25:07', '2019-12-16 02:30:38',...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>HunterSThomson2</td>\n",
       "      <td>dobsky33</td>\n",
       "      <td>1</td>\n",
       "      <td>['DelevoyeGate', 'Delevoye']</td>\n",
       "      <td>['2019-12-16 10:15:39']</td>\n",
       "      <td>['https://www.lemonde.fr/politique/article/201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>HunterSThomson2</td>\n",
       "      <td>philippe_dormoy</td>\n",
       "      <td>1</td>\n",
       "      <td>['Allemagne', 'Merkel', 'immigration', 'europé...</td>\n",
       "      <td>['2019-12-14 17:57:28']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ChWilhou</td>\n",
       "      <td>F_Desouche</td>\n",
       "      <td>1</td>\n",
       "      <td>['Toulouse', 'crèche']</td>\n",
       "      <td>['2019-12-15 00:59:35']</td>\n",
       "      <td>['http://www.fdesouche.com/1312053-toulouse-la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FSeclit</td>\n",
       "      <td>aoudairiou</td>\n",
       "      <td>1</td>\n",
       "      <td>['interview', 'misogyne', 'moustache', 'urinoir']</td>\n",
       "      <td>['2019-12-16 07:39:41']</td>\n",
       "      <td>['https://twitter.com/aoudairiou/status/120613...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0             user          mention  weight  \\\n",
       "0            0        A_Addams_  AllisonLeeSolin       1   \n",
       "1            1        A_Addams_       Babs_cat33       1   \n",
       "2            2        A_Addams_   InstitutILIADE       1   \n",
       "3            3        A_Addams_      RenaudCamus       1   \n",
       "4            4        A_Addams_  RevueInAnalysis       1   \n",
       "5            5        A_Addams_  TheeAndroidRulz       1   \n",
       "6            6        A_Addams_      aires_aston       1   \n",
       "7            7        A_Addams_       rick_hoyer       1   \n",
       "8            8        A_Addams_       truckster1       1   \n",
       "4            4  S_o_l_s_t_i_c_e    Chris_A10_USA       1   \n",
       "7            7  S_o_l_s_t_i_c_e       F_Desouche       1   \n",
       "17          17  S_o_l_s_t_i_c_e    ValentiniBack       6   \n",
       "28          28  S_o_l_s_t_i_c_e         jgremise       1   \n",
       "74          74     SainteOraora   rosylilacpetal       1   \n",
       "2            2    KimJongUnique       F_Desouche       1   \n",
       "35          35  TimeMachine1980  PresidenceNiger       1   \n",
       "38          38  TimeMachine1980     ROM1LeFrancc      12   \n",
       "1            1      T_Nenninger     AiphanMarcel       1   \n",
       "12          12      T_Nenninger      Drmartyufml       1   \n",
       "0            0  tprincedelamour            CNEWS       1   \n",
       "9            9  HunterSThomson2          FSeclit       3   \n",
       "54          54  HunterSThomson2         dobsky33       1   \n",
       "68          68  HunterSThomson2  philippe_dormoy       1   \n",
       "1            1         ChWilhou       F_Desouche       1   \n",
       "0            0          FSeclit       aoudairiou       1   \n",
       "\n",
       "                                             hashtags  \\\n",
       "0                     ['cdnpoli', 'debatimmigration']   \n",
       "1                          ['blacktwitter', 'racism']   \n",
       "2                          ['blacktwitter', 'racism']   \n",
       "3                          ['blacktwitter', 'racism']   \n",
       "4                     ['cdnpoli', 'debatimmigration']   \n",
       "5                           ['nrc', 'americadivided']   \n",
       "6                           ['nrc', 'americadivided']   \n",
       "7                     ['cdnpoli', 'debatimmigration']   \n",
       "8                           ['nrc', 'americadivided']   \n",
       "4                  ['Ireland', 'migrant', 'insanity']   \n",
       "7                             ['Belgique', 'Berchem']   \n",
       "17                 ['GregoireJunod', 'GregoireJunod']   \n",
       "28                          ['Occitanie', 'Toulouse']   \n",
       "74  ['jjbafanart', 'sbr', 'steelballrun', 'gyjo', ...   \n",
       "2                              ['Morbihan', 'préfet']   \n",
       "35                              ['Niamey', 'G5Sahel']   \n",
       "38               ['MissFrance2020', 'MissFrance2020']   \n",
       "1                       ['SibethNdiaye', 'retraites']   \n",
       "12          ['ReformeRetraites', 'RetraiteParPoints']   \n",
       "0                                  ['Dignes', 'Noel']   \n",
       "9     ['interview', 'misogyne', 'moustache', 'MeToo']   \n",
       "54                       ['DelevoyeGate', 'Delevoye']   \n",
       "68  ['Allemagne', 'Merkel', 'immigration', 'europé...   \n",
       "1                              ['Toulouse', 'crèche']   \n",
       "0   ['interview', 'misogyne', 'moustache', 'urinoir']   \n",
       "\n",
       "                                                 date  \\\n",
       "0                             ['2019-12-14 23:21:01']   \n",
       "1                             ['2019-12-15 23:21:01']   \n",
       "2                             ['2019-12-15 23:21:01']   \n",
       "3                             ['2019-12-15 23:21:01']   \n",
       "4                             ['2019-12-14 23:21:01']   \n",
       "5                             ['2019-12-15 11:21:03']   \n",
       "6                             ['2019-12-15 11:21:03']   \n",
       "7                             ['2019-12-14 23:21:01']   \n",
       "8                             ['2019-12-15 11:21:03']   \n",
       "4                             ['2019-12-15 21:25:47']   \n",
       "7                             ['2019-12-14 23:16:29']   \n",
       "17  ['2019-12-16 07:25:10', '2019-12-16 07:24:36',...   \n",
       "28                            ['2019-12-14 23:14:09']   \n",
       "74                            ['2019-12-15 19:21:33']   \n",
       "2                             ['2019-12-15 22:09:36']   \n",
       "35                            ['2019-12-15 15:52:42']   \n",
       "38  ['2019-12-15 15:36:18', '2019-12-15 15:35:51',...   \n",
       "1                             ['2019-12-15 22:40:44']   \n",
       "12                            ['2019-12-14 14:02:20']   \n",
       "0                             ['2019-12-14 18:07:27']   \n",
       "9   ['2019-12-16 09:25:07', '2019-12-16 02:30:38',...   \n",
       "54                            ['2019-12-16 10:15:39']   \n",
       "68                            ['2019-12-14 17:57:28']   \n",
       "1                             ['2019-12-15 00:59:35']   \n",
       "0                             ['2019-12-16 07:39:41']   \n",
       "\n",
       "                                                 urls  \n",
       "0   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "1   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "2   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "3   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "4   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "5   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "6   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "7   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "8   ['https://paper.li/A_Addams_/1421662602?editio...  \n",
       "4                                                  []  \n",
       "7                                                  []  \n",
       "17  ['https://lesobservateurs.ch/2019/12/16/italie...  \n",
       "28                                                 []  \n",
       "74  ['https://twitter.com/notcorrectjjba/status/12...  \n",
       "2                                                  []  \n",
       "35                                                 []  \n",
       "38                                                 []  \n",
       "1                                                  []  \n",
       "12                                                 []  \n",
       "0   ['https://www.cnews.fr/videos/france/2019-12-1...  \n",
       "9                                                  []  \n",
       "54  ['https://www.lemonde.fr/politique/article/201...  \n",
       "68                                                 []  \n",
       "1   ['http://www.fdesouche.com/1312053-toulouse-la...  \n",
       "0   ['https://twitter.com/aoudairiou/status/120613...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display edges with number of hashtags >1\n",
    "edge_df[edge_df['hashtags'].apply(lambda x : len(x.split()))>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def graph_from_edgeslist(edge_df,degree_min):\n",
    "    print('Creating the graph fro th edge list')\n",
    "    G = nx.from_pandas_edgelist(edge_df,source='user',target='mention', edge_attr=['weight','hashtags','date','urls'])\n",
    "    print('Nb of nodes:',G.number_of_nodes())\n",
    "    # Drop\n",
    "    remove = [node for node,degree in dict(G.degree()).items() if degree < degree_min]\n",
    "    G.remove_nodes_from(remove)\n",
    "    print('Nb of nodes after removing less connected nodes:',G.number_of_nodes())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the graph fro th edge list\n",
      "Nb of nodes: 431\n",
      "Nb of nodes after removing less connected nodes: 38\n",
      "removed 1 isolated nodes.\n"
     ]
    }
   ],
   "source": [
    "DEGREE_MIN = 3 # Minimal number of connections in the graph\n",
    "\n",
    "G = graph_from_edgeslist(edge_df,DEGREE_MIN)\n",
    "isolates = list(nx.isolates(G))\n",
    "G.remove_nodes_from(isolates)\n",
    "print('removed {} isolated nodes.'.format(len(isolates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of partitions: 4\n"
     ]
    }
   ],
   "source": [
    "#first compute the best partition\n",
    "partition = community.best_partition(G)\n",
    "nx.set_node_attributes(G,partition,name='community')\n",
    "nb_partitions = max(partition.values())+1\n",
    "print('Nb of partitions:',nb_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphname = 'multiusersgraph'\n",
    "#graphname = 'GBRgraph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote immigration_reduced191216/multiusersgraph_t2_graph.gexf\n"
     ]
    }
   ],
   "source": [
    "# Save the graph\n",
    "graphfilename = data_path + graphname + '_t' + str(thres) +'_graph.gexf'\n",
    "nx.write_gexf(G,graphfilename)\n",
    "print('Wrote',graphfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags, dates and urls\n",
    "Hashtags, dates and urls are on the edges of the network.\n",
    "We can get the most common hashtags within a community and also betwenn communities using the edges that connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast # convert string to list\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the hashtags for each community and inter-communities\n",
    "tags_dic = {}\n",
    "dates_dic = {}\n",
    "url_dic = {}\n",
    "for node1,node2,data in G.edges(data=True):\n",
    "    if node1 == node2:\n",
    "        print('Self edge',node1)\n",
    "    n1_com = G.nodes[node1]['community']\n",
    "    n2_com = G.nodes[node2]['community']\n",
    "    new_key = str(n1_com) + '-' + str(n2_com) # intra / inter community code\n",
    "    # Convert string to list\n",
    "    x = ast.literal_eval(data['hashtags'])\n",
    "    d = ast.literal_eval(data['date'])\n",
    "    u = ast.literal_eval(data['urls'])\n",
    "    keywords = [n.strip() for n in x]\n",
    "    date_list = [n.strip() for n in d]\n",
    "    urls = [n.strip() for n in u]\n",
    "    if new_key not in tags_dic:\n",
    "        tags_dic[new_key] = keywords\n",
    "        dates_dic[new_key] = date_list\n",
    "        url_dic[new_key] = urls\n",
    "    else:\n",
    "        tags_dic[new_key] += keywords \n",
    "        dates_dic[new_key] += date_list\n",
    "        url_dic[new_key] += urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the most common hashtags in communities and inter communities\n",
    "#for key in tags_dic:\n",
    "#    most_common = Counter(tags_dic[key]).most_common(5)\n",
    "#    print(key)\n",
    "#    print(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meantime(date_list):\n",
    "    d_list = [ datetime.datetime.strptime(dt,'%Y-%m-%d %H:%M:%S') for dt in date_list]\n",
    "    second_list = [x.timestamp() for x in d_list]\n",
    "    meand = np.mean(second_list)\n",
    "    stdd = np.std(second_list)\n",
    "    return datetime.datetime.fromtimestamp(meand),datetime.timedelta(seconds=stdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a table with time and hashtags for each community\n",
    "comm_list = []\n",
    "for key in np.arange(nb_partitions):\n",
    "    keykey = str(key)+ '-' +str(key)\n",
    "    most_common = Counter(tags_dic[keykey]).most_common(5)\n",
    "    meandate,stddate = compute_meantime(dates_dic[keykey])\n",
    "    #print('Community',key)\n",
    "    #print(most_common)\n",
    "    #print('Average date: {} and std deviation: {} days'.format(meandate.date(),stddate.days))\n",
    "    comm_dic = {'Community':key, 'Average date':meandate.date(), 'Deviation (days)':stddate.days}\n",
    "    for htag_nb in range(5): # filling the table with the hashtags\n",
    "        if htag_nb < len(most_common):\n",
    "            comm_dic['hashtag'+str(htag_nb)] = most_common[htag_nb][0]\n",
    "        else:\n",
    "            comm_dic['hashtag'+str(htag_nb)] = ''\n",
    "    comm_list.append(comm_dic)\n",
    "community_table = pd.DataFrame(comm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Community</th>\n",
       "      <th>Average date</th>\n",
       "      <th>Deviation (days)</th>\n",
       "      <th>hashtag0</th>\n",
       "      <th>hashtag1</th>\n",
       "      <th>hashtag2</th>\n",
       "      <th>hashtag3</th>\n",
       "      <th>hashtag4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-15</td>\n",
       "      <td>0</td>\n",
       "      <td>Belgique</td>\n",
       "      <td>Berchem</td>\n",
       "      <td>MissFrance</td>\n",
       "      <td>Toulouse</td>\n",
       "      <td>crèche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-12-15</td>\n",
       "      <td>0</td>\n",
       "      <td>interview</td>\n",
       "      <td>misogyne</td>\n",
       "      <td>moustache</td>\n",
       "      <td>GretaIceberg</td>\n",
       "      <td>MeToo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-15</td>\n",
       "      <td>0</td>\n",
       "      <td>MissFrance</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-12-15</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Community Average date  Deviation (days)    hashtag0  hashtag1    hashtag2  \\\n",
       "0          0   2019-12-15                 0    Belgique   Berchem  MissFrance   \n",
       "1          1   2019-12-15                 0   interview  misogyne   moustache   \n",
       "2          2   2019-12-15                 0  MissFrance                         \n",
       "3          3   2019-12-15                 0                                     \n",
       "\n",
       "       hashtag3 hashtag4  \n",
       "0      Toulouse   crèche  \n",
       "1  GretaIceberg    MeToo  \n",
       "2                         \n",
       "3                         "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the url of each cluster and inter-clusters\n",
    "urltocomm = []\n",
    "for key in url_dic:\n",
    "    for url in url_dic[key]:\n",
    "        urltocomm.append([url,key,1])\n",
    "url_table = pd.DataFrame(urltocomm, columns=['url','Community','Occurence'])\n",
    "url_table = url_table.groupby(['url','Community']).agg(Occurence=('Occurence',sum))\n",
    "url_table = url_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all bit.ly url by the correct one\n",
    "import requests\n",
    "\n",
    "session = requests.Session()  # so connections are recycled\n",
    "\n",
    "for index, row in url_table.iterrows():\n",
    "    url = row['url']\n",
    "    if 'bit.ly' in url:\n",
    "        resp = session.head(url, allow_redirects=True)\n",
    "        url_table.loc[index,'url'] = resp.url\n",
    "        #print(resp.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the references to twitter web site\n",
    "twitterrowindices = url_table[url_table['url'].str.contains('twitter.com')].index\n",
    "filtered_url_table = url_table.drop(twitterrowindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the intra community links\n",
    "dropindices = []\n",
    "for index, row in filtered_url_table.iterrows():\n",
    "    if row['Community'][0] != row['Community'][-1]:\n",
    "        dropindices.append(index)\n",
    "    else: # modify the entry\n",
    "        filtered_url_table.loc[index,'Community'] = row['Community'][0]\n",
    "filtered_url_table = filtered_url_table.drop(dropindices)\n",
    "filtered_url_table.reset_index(inplace=True)\n",
    "filtered_url_table.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort users by community and save in a excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort users by community and store their node degree (importance)\n",
    "community_nodes = {}\n",
    "for node,data in G.nodes(data=True):\n",
    "    community_nb = data['community']\n",
    "    if  community_nb not in community_nodes:\n",
    "        community_nodes[community_nb] = [(node, G.degree(node))]\n",
    "    else:\n",
    "        community_nodes[community_nb].append((node, G.degree(node)))\n",
    "\n",
    "\n",
    "# Display the exmaple of community c_idx\n",
    "#c_idx = 0\n",
    "#ddf = pd.DataFrame(community_nodes[c_idx],columns=['User','Degree'])\n",
    "#print('list of most connected users in community',c_idx)\n",
    "#ddf.sort_values(by='Degree',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to an excel file\n",
    "with pd.ExcelWriter(data_path + 'graph_infos.xlsx') as writer:\n",
    "    for community_nb in community_nodes:\n",
    "        ddf = pd.DataFrame(community_nodes[community_nb],columns=['User','Degree'])\n",
    "        ddf = ddf.sort_values(by='Degree',ascending=False)#.head(20)\n",
    "        ddf.to_excel(writer, sheet_name='Community_' + str(community_nb),index=False)\n",
    "    community_table.to_excel(writer, sheet_name='Hashtags',index=False)\n",
    "    users_df.to_excel(writer, sheet_name='Initial_users_details',index=False)\n",
    "    filtered_url_table.to_excel(writer, sheet_name='List_of_urls',index=False)\n",
    "    # Set the column width\n",
    "    column_width = 25\n",
    "    for sheet in writer.sheets: \n",
    "        worksheet = writer.sheets[sheet]\n",
    "        for col in ['A','B','C','D','E','F','G','H']:\n",
    "            worksheet.column_dimensions[col].width = column_width\n",
    "    writer.sheets['List_of_urls'].column_dimensions['A'].width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
