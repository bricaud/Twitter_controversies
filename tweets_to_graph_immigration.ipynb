{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Twython class\n",
    "from twython import Twython\n",
    "import json\n",
    "\n",
    "# Load credentials from json file\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)\n",
    "\n",
    "# Instantiate an object\n",
    "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#username = 'templivs'\n",
    "#username_list  = ['GilbertCollard','dav_dec','Carbongate','bcassoret',\n",
    "#                  'Electroversenet','thinkfree55', 'KlassLib','sauvonsleclimat']\n",
    "\n",
    "username_list = ['francisrichard','MazdaArtaxerxes','templivs','prontipronto',\n",
    "                'Chabadalala','cocktail2Funk','HopitalC',\n",
    "                'riva_vitale','Remifasol57','AitiDouze', 'QAnonAustria1', 'gotteswerk2411']\n",
    "\n",
    "swiss_accounts = ['KlimaschutzCH', 'GrueneCH', 'proclimCH', 'EperonP', 'MathiasTemujin',\n",
    "                  'klimastreik', 'AlimEquitables', 'ProNaturaSuisse', 'vertliberaux', 'Munsterma',\n",
    "                  'bourg_d', 'LesVertsSuisses', 'ClimatSuisse', 'gpsuisse', 'IliasPanchard', 'ATE_Suisse']\n",
    "\n",
    "immigration = ['Kalvingrad1291','democratesuisse','VigilanceIslam','lioneljonson01','ChWilhou',\n",
    "               'HunterSThomson2','A_Addams_','ObservateursCH','JuanCandida','novopress']\n",
    "\n",
    "username_list = immigration\n",
    "# create the path to save the experiment indexed with a date\n",
    "today = date.today()\n",
    "date_string = today.strftime(\"%y%m%d\")\n",
    "print(\"date string =\", date_string)\n",
    "\n",
    "#date_string = '191128'\n",
    "\n",
    "data_path = 'immigration_reduced' + date_string+ '/'\n",
    "#get_tweets = python_tweets.get_user_timeline(screen_name = username,  \n",
    "#                                           count = 200, include_rts = True)\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n",
    "    print('Path created:',data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pysad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 2 # minimal number of mentions to keep\n",
    "max_day_old = 2 # number max of days in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_list(python_tweets, data_path, username, thres=3, max_day_old=None):\n",
    "    users_dic = {'username':[], 'Nb_diff_mentions': []}\n",
    "    print('Collecting the tweets for the last {} days.'.format(max_day_old))\n",
    "    new_users_list = []\n",
    "    for user in username_list:\n",
    "        mentions = create_user_edgelist(python_tweets, data_path, user, thres=thres, max_day_old=max_day_old)\n",
    "        if not mentions.empty:\n",
    "            users_mentioned = mentions['mention'][mentions['weight']>thres]\n",
    "            #users_mentioned = users_mentioned.unique() # not sure this is useful\n",
    "            new_users_list += users_mentioned.tolist()\n",
    "        users_dic['username'].append(user)\n",
    "        users_dic['Nb_diff_mentions'].append(len(mentions))\n",
    "    users_df = pd.DataFrame(users_dic)\n",
    "    return new_users_list,users_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_user_edgelist(python_tweets, data_path, username, thres, max_day_old):\n",
    "\t# Process the user username and its mentioned users\n",
    "\t# save in a file the edgelist for the user and each mentioned user\n",
    "\n",
    "\t# initial user\n",
    "\tprint('Processing',username)\n",
    "\t#try:\n",
    "\tmention_grouped,mgl = pysad.collect_user_mention(username,python_tweets,data_path, max_day_old=max_day_old)\n",
    "\t#except:\n",
    "\t#    print('exception catched on user {} !!!!!!!!!!!!'.format(username))\n",
    "\t#    return\n",
    "\tif mention_grouped.empty:\n",
    "\t\tprint('Empty tweet list. Processing stopped for user ',username)\n",
    "\t\treturn mention_grouped\n",
    "\tmentionfilename = data_path + username + '_mentions' +'_t' +str(thres)+'.csv'\n",
    "\tprint('Writing {} tweets in {}.'.format(len(mention_grouped),mentionfilename))\n",
    "\tmention_grouped.to_csv(mentionfilename)\n",
    "\t#nb_mentions = len(mention_grouped)\n",
    "\t#print('User {} done. Nb different mentions: {}'.format(username,nb_mentions))\n",
    "\treturn mention_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users_dic = {'username':[], 'Nb_mentions': [], 'mentions_of_mentions': []}\n",
    "print('Collecting the tweets for the last {} days.'.format(max_day_old))\n",
    "exploration_depth = 2\n",
    "total_username_list = username_list\n",
    "for depth in range(exploration_depth):\n",
    "    print('')\n",
    "    print('******* Processing users at {}-hop distance *******'.format(depth))\n",
    "    new_users_list,users_df = process_user_list(python_tweets, data_path, username_list, thres=thres, max_day_old=max_day_old)\n",
    "    #New users to collect:\n",
    "    username_list = list(set(new_users_list).difference(set(total_username_list))) # remove the one already collected\n",
    "    total_username_list += username_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of users collected:')\n",
    "print(len(total_username_list),len(set(total_username_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved data into an edge table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "edge_df = pd.DataFrame()\n",
    "for filename in glob.glob(data_path + '*_mentions' +'_t' +str(thres)+ '.csv'):\n",
    "    new_edge_df = pd.read_csv(filename)\n",
    "    print('{} with {} tweets.'.format(filename,len(new_edge_df)))\n",
    "    edge_df = edge_df.append(new_edge_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display edges with number of hashtags >1\n",
    "edge_df[edge_df['hashtags'].apply(lambda x : len(x.split()))>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def graph_from_edgeslist(edge_df,degree_min):\n",
    "    print('Creating the graph fro th edge list')\n",
    "    G = nx.from_pandas_edgelist(edge_df,source='user',target='mention', edge_attr=['weight','hashtags','date','urls'])\n",
    "    print('Nb of nodes:',G.number_of_nodes())\n",
    "    # Drop\n",
    "    remove = [node for node,degree in dict(G.degree()).items() if degree < degree_min]\n",
    "    G.remove_nodes_from(remove)\n",
    "    print('Nb of nodes after removing less connected nodes:',G.number_of_nodes())\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEGREE_MIN = 3 # Minimal number of connections in the graph\n",
    "\n",
    "G = graph_from_edgeslist(edge_df,DEGREE_MIN)\n",
    "isolates = list(nx.isolates(G))\n",
    "G.remove_nodes_from(isolates)\n",
    "print('removed {} isolated nodes.'.format(len(isolates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first compute the best partition\n",
    "partition = community.best_partition(G)\n",
    "nx.set_node_attributes(G,partition,name='community')\n",
    "nb_partitions = max(partition.values())+1\n",
    "print('Nb of partitions:',nb_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphname = 'multiusersgraph'\n",
    "#graphname = 'GBRgraph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph\n",
    "graphfilename = data_path + graphname + '_t' + str(thres) +'_graph.gexf'\n",
    "nx.write_gexf(G,graphfilename)\n",
    "print('Wrote',graphfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags, dates and urls\n",
    "Hashtags, dates and urls are on the edges of the network.\n",
    "We can get the most common hashtags within a community and also betwenn communities using the edges that connect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast # convert string to list\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the hashtags for each community and inter-communities\n",
    "tags_dic = {}\n",
    "dates_dic = {}\n",
    "url_dic = {}\n",
    "for node1,node2,data in G.edges(data=True):\n",
    "    if node1 == node2:\n",
    "        print('Self edge',node1)\n",
    "    n1_com = G.nodes[node1]['community']\n",
    "    n2_com = G.nodes[node2]['community']\n",
    "    new_key = str(n1_com) + '-' + str(n2_com) # intra / inter community code\n",
    "    # Convert string to list\n",
    "    x = ast.literal_eval(data['hashtags'])\n",
    "    d = ast.literal_eval(data['date'])\n",
    "    u = ast.literal_eval(data['urls'])\n",
    "    keywords = [n.strip() for n in x]\n",
    "    date_list = [n.strip() for n in d]\n",
    "    urls = [n.strip() for n in u]\n",
    "    if new_key not in tags_dic:\n",
    "        tags_dic[new_key] = keywords\n",
    "        dates_dic[new_key] = date_list\n",
    "        url_dic[new_key] = urls\n",
    "    else:\n",
    "        tags_dic[new_key] += keywords \n",
    "        dates_dic[new_key] += date_list\n",
    "        url_dic[new_key] += urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the most common hashtags in communities and inter communities\n",
    "#for key in tags_dic:\n",
    "#    most_common = Counter(tags_dic[key]).most_common(5)\n",
    "#    print(key)\n",
    "#    print(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_meantime(date_list):\n",
    "    d_list = [ datetime.datetime.strptime(dt,'%Y-%m-%d %H:%M:%S') for dt in date_list]\n",
    "    second_list = [x.timestamp() for x in d_list]\n",
    "    meand = np.mean(second_list)\n",
    "    stdd = np.std(second_list)\n",
    "    return datetime.datetime.fromtimestamp(meand),datetime.timedelta(seconds=stdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a table with time and hashtags for each community\n",
    "comm_list = []\n",
    "for key in np.arange(nb_partitions):\n",
    "    keykey = str(key)+ '-' +str(key)\n",
    "    most_common = Counter(tags_dic[keykey]).most_common(5)\n",
    "    meandate,stddate = compute_meantime(dates_dic[keykey])\n",
    "    #print('Community',key)\n",
    "    #print(most_common)\n",
    "    #print('Average date: {} and std deviation: {} days'.format(meandate.date(),stddate.days))\n",
    "    comm_dic = {'Community':key, 'Average date':meandate.date(), 'Deviation (days)':stddate.days}\n",
    "    for htag_nb in range(5): # filling the table with the hashtags\n",
    "        if htag_nb < len(most_common):\n",
    "            comm_dic['hashtag'+str(htag_nb)] = most_common[htag_nb][0]\n",
    "        else:\n",
    "            comm_dic['hashtag'+str(htag_nb)] = ''\n",
    "    comm_list.append(comm_dic)\n",
    "community_table = pd.DataFrame(comm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with the url of each cluster and inter-clusters\n",
    "urltocomm = []\n",
    "for key in url_dic:\n",
    "    for url in url_dic[key]:\n",
    "        urltocomm.append([url,key,1])\n",
    "url_table = pd.DataFrame(urltocomm, columns=['url','Community','Occurence'])\n",
    "url_table = url_table.groupby(['url','Community']).agg(Occurence=('Occurence',sum))\n",
    "url_table = url_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all bit.ly url by the correct one\n",
    "import requests\n",
    "\n",
    "session = requests.Session()  # so connections are recycled\n",
    "\n",
    "for index, row in url_table.iterrows():\n",
    "    url = row['url']\n",
    "    if 'bit.ly' in url:\n",
    "        resp = session.head(url, allow_redirects=True)\n",
    "        url_table.loc[index,'url'] = resp.url\n",
    "        #print(resp.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the references to twitter web site\n",
    "twitterrowindices = url_table[url_table['url'].str.contains('twitter.com')].index\n",
    "filtered_url_table = url_table.drop(twitterrowindices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the intra community links\n",
    "dropindices = []\n",
    "for index, row in filtered_url_table.iterrows():\n",
    "    if row['Community'][0] != row['Community'][-1]:\n",
    "        dropindices.append(index)\n",
    "    else: # modify the entry\n",
    "        filtered_url_table.loc[index,'Community'] = row['Community'][0]\n",
    "filtered_url_table = filtered_url_table.drop(dropindices)\n",
    "filtered_url_table.reset_index(inplace=True)\n",
    "filtered_url_table.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort users by community and save in a excel sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort users by community and store their node degree (importance)\n",
    "community_nodes = {}\n",
    "for node,data in G.nodes(data=True):\n",
    "    community_nb = data['community']\n",
    "    if  community_nb not in community_nodes:\n",
    "        community_nodes[community_nb] = [(node, G.degree(node))]\n",
    "    else:\n",
    "        community_nodes[community_nb].append((node, G.degree(node)))\n",
    "\n",
    "\n",
    "# Display the exmaple of community c_idx\n",
    "#c_idx = 0\n",
    "#ddf = pd.DataFrame(community_nodes[c_idx],columns=['User','Degree'])\n",
    "#print('list of most connected users in community',c_idx)\n",
    "#ddf.sort_values(by='Degree',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to an excel file\n",
    "with pd.ExcelWriter(data_path + 'graph_infos.xlsx') as writer:\n",
    "    for community_nb in community_nodes:\n",
    "        ddf = pd.DataFrame(community_nodes[community_nb],columns=['User','Degree'])\n",
    "        ddf = ddf.sort_values(by='Degree',ascending=False)#.head(20)\n",
    "        ddf.to_excel(writer, sheet_name='Community_' + str(community_nb),index=False)\n",
    "    community_table.to_excel(writer, sheet_name='Hashtags',index=False)\n",
    "    users_df.to_excel(writer, sheet_name='Initial_users_details',index=False)\n",
    "    filtered_url_table.to_excel(writer, sheet_name='List_of_urls',index=False)\n",
    "    # Set the column width\n",
    "    column_width = 25\n",
    "    for sheet in writer.sheets: \n",
    "        worksheet = writer.sheets[sheet]\n",
    "        for col in ['A','B','C','D','E','F','G','H']:\n",
    "            worksheet.column_dimensions[col].width = column_width\n",
    "    writer.sheets['List_of_urls'].column_dimensions['A'].width = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
